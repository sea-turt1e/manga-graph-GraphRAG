import json
import logging
import os
import time
from typing import Any, Callable, Dict, List, Optional

import requests
import streamlit as st
from dotenv import load_dotenv

from prompts.manga_prompts import GraphRAGPrompts, StandardMangaPrompts
from retry_utils import request_with_retry

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

st.set_page_config(
    page_title="GraphRAGã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆãƒ‡ãƒ¢", page_icon="ğŸ“š", layout="wide", initial_sidebar_state="collapsed"
)
load_dotenv()

# Optional API key for backend
BACKEND_API_KEY = os.getenv("BACKEND_API_KEY", "").strip()
API_BASE = os.getenv("API_BASE", "http://localhost:8000")

# API Endpoints
# çµ±åˆAPI (1-6, 7-8, 11-13)
GRAPH_CASCADE_ENDPOINT = f"{API_BASE}/api/v1/manga-anime-neo4j/graph/cascade"
VECTOR_SIMILARITY_MULTI_ENDPOINT = f"{API_BASE}/api/v1/manga-anime-neo4j/vector/similarity/multi"
RELATED_GRAPHS_BATCH_ENDPOINT = f"{API_BASE}/api/v1/manga-anime-neo4j/related-graphs/batch"
MAGAZINES_WORK_GRAPH_ENDPOINT = f"{API_BASE}/api/v1/manga-anime-neo4j/magazines/work-graph"
TEXT_GEN_ENDPOINT = f"{API_BASE}/text-generation/generate"


def _auth_headers(extra: dict | None = None) -> dict:
    headers: dict = {}
    if BACKEND_API_KEY:
        headers["Authorization"] = f"Bearer {BACKEND_API_KEY}"
    headers["X-API-Key"] = BACKEND_API_KEY
    if extra:
        headers.update(extra)
    return headers


# =============================================================================
# Backend API Functions for GraphRAG (çµ±åˆAPIä½¿ç”¨)
# =============================================================================


def search_graph_cascade(query: str, limit: int = 3, languages: str = "japanese,english") -> Dict[str, Any]:
    """
    ã‚°ãƒ©ãƒ•æ¤œç´¢çµ±åˆAPIå‘¼ã³å‡ºã— (1-6 çµ±åˆ)
    japanese/simple -> japanese/fulltext -> japanese/ranked -> english/simple -> english/fulltext -> english/ranked
    ã‚’1å›ã®APIå‘¼ã³å‡ºã—ã§å®Ÿè¡Œ
    """
    params = {
        "q": query,
        "limit": limit,
        "languages": languages,
        "include_hentai": False,
    }
    try:
        r = request_with_retry(
            "GET",
            GRAPH_CASCADE_ENDPOINT,
            params=params,
            headers=_auth_headers(),
            timeout=60,
        )
        r.raise_for_status()
        return r.json()
    except Exception as e:
        logger.warning("Graph cascade search error: %s", e)
        return {}


def get_related_graphs_batch(
    author_node_id: str | None = None,
    magazine_node_id: str | None = None,
    publisher_node_id: str | None = None,
    author_limit: int = 5,
    magazine_limit: int = 5,
    publisher_limit: int = 3,
    reference_work_id: str | None = None,
    exclude_magazine_id: str | None = None,
) -> Dict[str, Any]:
    """
    é–¢é€£ã‚°ãƒ©ãƒ•ä¸€æ‹¬å–å¾—APIå‘¼ã³å‡ºã— (11-13 çµ±åˆ)
    è‘—è€…ã®ä»–ä½œå“ã€é›‘èªŒã®ä»–ä½œå“ã€å‡ºç‰ˆç¤¾ã®ä»–é›‘èªŒã‚’1å›ã®APIå‘¼ã³å‡ºã—ã§å–å¾—
    """
    body = {
        "author_node_id": author_node_id,
        "magazine_node_id": magazine_node_id,
        "publisher_node_id": publisher_node_id,
        "author_limit": author_limit,
        "magazine_limit": magazine_limit,
        "publisher_limit": publisher_limit,
        "include_hentai": False,
    }
    if reference_work_id:
        body["reference_work_id"] = reference_work_id
    if exclude_magazine_id:
        body["exclude_magazine_id"] = exclude_magazine_id
    
    try:
        r = request_with_retry(
            "POST",
            RELATED_GRAPHS_BATCH_ENDPOINT,
            json=body,
            headers=_auth_headers({"Content-Type": "application/json"}),
            timeout=60,
        )
        r.raise_for_status()
        return r.json()
    except Exception as e:
        logger.warning("Related graphs batch error: %s", e)
        return {}


def search_vector_similarity_multi(
    query: str,
    embedding_types: List[str] | None = None,
    limit: int = 10,
    threshold: float = 0.3,
) -> Dict[str, Any]:
    """
    ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼æ¤œç´¢çµ±åˆAPIå‘¼ã³å‡ºã— (7-8 çµ±åˆ)
    title_en ã¨ title_ja ã§ã®æ¤œç´¢ã‚’1å›ã®APIå‘¼ã³å‡ºã—ã§å®Ÿè¡Œ
    çµæœã¯æ—¢ã«ãƒãƒ¼ã‚¸ãƒ»é‡è¤‡æ’é™¤ãƒ»ã‚½ãƒ¼ãƒˆæ¸ˆã¿
    """
    if embedding_types is None:
        embedding_types = ["title_en", "title_ja"]
    
    body = {
        "query": query,
        "embedding_types": embedding_types,
        "embedding_dims": 256,
        "limit": limit,
        "threshold": threshold,
        "include_hentai": False,
    }
    try:
        r = request_with_retry(
            "POST",
            VECTOR_SIMILARITY_MULTI_ENDPOINT,
            json=body,
            headers=_auth_headers({"Content-Type": "application/json"}),
            timeout=60,
        )
        r.raise_for_status()
        result = r.json()
        logger.info(f"Vector similarity multi search: {len(result.get('results', []))} results")
        return result
    except Exception as e:
        logger.warning("Vector similarity multi search error: %s", e)
        return {}


def get_magazines_work_graph(magazine_ids: List[str], work_limit: int = 3, reference_work_id: str | None = None) -> Dict[str, Any]:
    """è¤‡æ•°é›‘èªŒã®ä½œå“ã‚°ãƒ©ãƒ•ã‚’å–å¾—"""
    # ç©ºã®å ´åˆã¯æ—©æœŸãƒªã‚¿ãƒ¼ãƒ³
    if not magazine_ids:
        logger.warning("get_magazines_work_graph: magazine_ids is empty")
        return {}
    
    body = {
        "magazine_element_ids": magazine_ids,  # APIã‚¹ã‚­ãƒ¼ãƒã«åˆã‚ã›ãŸãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å
        "work_limit": work_limit,
        "include_hentai": False,
    }
    if reference_work_id:
        body["reference_work_id"] = reference_work_id
    
    logger.info(f"Magazines work graph request body: {body}")
    
    try:
        r = request_with_retry(
            "POST",
            MAGAZINES_WORK_GRAPH_ENDPOINT,
            json=body,
            headers=_auth_headers({"Content-Type": "application/json"}),
            timeout=60,
        )
        r.raise_for_status()
        return r.json()
    except requests.exceptions.HTTPError as e:
        # ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è©³ç´°ã‚’ãƒ­ã‚°å‡ºåŠ›
        try:
            error_detail = e.response.json()
            logger.warning("Magazines work graph error: %s - Detail: %s", e, error_detail)
        except Exception:
            logger.warning("Magazines work graph error: %s - Response: %s", e, e.response.text)
        return {}
    except Exception as e:
        logger.warning("Magazines work graph error: %s", e)
        return {}


def extract_ids_from_graph(graph: Dict[str, Any]) -> Dict[str, Any]:
    """ã‚°ãƒ©ãƒ•ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒãƒ¼ãƒ‰IDã‚’æŠ½å‡º"""
    result = {
        "work_id": None,
        "work_title": None,
        "author_ids": [],
        "magazine_ids": [],
        "publisher_ids": [],
    }
    
    nodes = graph.get("nodes", []) or []
    edges = graph.get("edges", []) or graph.get("relationships", []) or []
    
    # ãƒãƒ¼ãƒ‰ã‚’ã‚¿ã‚¤ãƒ—åˆ¥ã«åˆ†é¡
    for node in nodes:
        node_type = node.get("type", "").lower()
        node_id = node.get("id") or node.get("elementId")
        
        if node_type == "work":
            if result["work_id"] is None:
                result["work_id"] = node_id
                # japanese_nameã‚’å„ªå…ˆçš„ã«ä½¿ç”¨
                props = node.get("properties", {})
                result["work_title"] = (
                    props.get("japanese_name") 
                    or props.get("title") 
                    or node.get("title") 
                    or node.get("label")
                )
        elif node_type == "author":
            if node_id and node_id not in result["author_ids"]:
                result["author_ids"].append(node_id)
        elif node_type == "magazine":
            if node_id and node_id not in result["magazine_ids"]:
                result["magazine_ids"].append(node_id)
        elif node_type == "publisher":
            if node_id and node_id not in result["publisher_ids"]:
                result["publisher_ids"].append(node_id)
    
    return result


def get_work_title(node: Dict[str, Any]) -> str:
    """Workãƒãƒ¼ãƒ‰ã‹ã‚‰æ¼«ç”»åã‚’å–å¾—ï¼ˆjapanese_nameã‚’å„ªå…ˆï¼‰"""
    props = node.get("properties", {})
    return (
        props.get("japanese_name")
        or props.get("title")
        or node.get("title")
        or node.get("label")
        or ""
    )


def perform_graph_search(query: str) -> tuple[Dict[str, Any], str]:
    """
    ã‚°ãƒ©ãƒ•æ¤œç´¢ã‚’å®Ÿè¡Œï¼ˆçµ±åˆAPIä½¿ç”¨ï¼‰
    1-6ã®æ¤œç´¢ã‚’1å›ã®APIå‘¼ã³å‡ºã—ã§å®Ÿè¡Œ
    
    Returns: (graph_response, search_mode_used)
    """
    result = search_graph_cascade(query, limit=3, languages="japanese,english")
    nodes = result.get("nodes", []) or []
    if nodes:
        logger.info(f"Graph cascade search found {len(nodes)} nodes")
        return result, "cascade"
    
    return {}, ""


def fetch_extended_graph_info(base_graph: Dict[str, Any]) -> Dict[str, Any]:
    """
    11-13: è¿½åŠ ã®ã‚°ãƒ©ãƒ•æƒ…å ±ã‚’å–å¾—ï¼ˆçµ±åˆAPIä½¿ç”¨ï¼‰
    11. è‘—è€…ã®ä»–ä½œå“
    12. é›‘èªŒã®ä»–ä½œå“
    13. å‡ºç‰ˆç¤¾ã®ä»–é›‘èªŒ
    14. ä»–é›‘èªŒã®ä½œå“ã‚°ãƒ©ãƒ•
    """
    ids = extract_ids_from_graph(base_graph)
    
    extended_info = {
        "base_graph": base_graph,
        "author_works": [],
        "magazine_works": [],
        "publisher_magazines": [],
        "other_magazines_works": [],
        "extracted_ids": ids,
    }
    
    work_id = ids.get("work_id")
    author_ids = ids.get("author_ids", [])
    magazine_ids = ids.get("magazine_ids", [])
    publisher_ids = ids.get("publisher_ids", [])
    
    # 11-13: é–¢é€£ã‚°ãƒ©ãƒ•ã‚’ä¸€æ‹¬å–å¾—ï¼ˆçµ±åˆAPIï¼‰
    author_node_id = author_ids[0] if author_ids else None
    magazine_node_id = magazine_ids[0] if magazine_ids else None
    publisher_node_id = publisher_ids[0] if publisher_ids else None
    exclude_mag = magazine_ids[0] if magazine_ids else None
    
    related_graphs = get_related_graphs_batch(
        author_node_id=author_node_id,
        magazine_node_id=magazine_node_id,
        publisher_node_id=publisher_node_id,
        author_limit=5,
        magazine_limit=5,
        publisher_limit=3,
        reference_work_id=work_id,
        exclude_magazine_id=exclude_mag,
    )
    
    # çµ±åˆAPIã®çµæœã‚’å¾“æ¥ã®å½¢å¼ã«å¤‰æ›
    if related_graphs.get("author_graph"):
        extended_info["author_works"].append(related_graphs["author_graph"])
    
    if related_graphs.get("magazine_graph"):
        extended_info["magazine_works"].append(related_graphs["magazine_graph"])
    
    if related_graphs.get("publisher_graph"):
        extended_info["publisher_magazines"].append(related_graphs["publisher_graph"])
        # ä»–é›‘èªŒIDã‚’åé›†
        other_magazine_ids = []
        for node in related_graphs["publisher_graph"].get("nodes", []):
            if node.get("type", "").lower() == "magazine":
                mag_id = node.get("id") or node.get("elementId")
                if mag_id and mag_id not in other_magazine_ids:
                    other_magazine_ids.append(mag_id)
        
        # 14. ä»–é›‘èªŒã®ä½œå“ã‚°ãƒ©ãƒ•
        if other_magazine_ids:
            other_works = get_magazines_work_graph(other_magazine_ids, work_limit=3, reference_work_id=work_id)
            if other_works:
                extended_info["other_magazines_works"].append(other_works)
    
    return extended_info


def build_graph_context_from_extended(extended_info: Dict[str, Any], query_title: str) -> str:
    """
    15: æ‹¡å¼µã‚°ãƒ©ãƒ•æƒ…å ±ã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—ã‚’æ§‹ç¯‰
    """
    lines: List[str] = []
    
    base_graph = extended_info.get("base_graph", {})
    ids = extended_info.get("extracted_ids", {})
    
    # åŸºæœ¬æƒ…å ±
    work_title = ids.get("work_title") or query_title
    lines.append(f"ã‚¯ã‚¨ãƒª: {work_title}")
    
    # è‘—è€…æƒ…å ±
    base_nodes = base_graph.get("nodes", []) or []
    authors = [n for n in base_nodes if n.get("type", "").lower() == "author"]
    magazines = [n for n in base_nodes if n.get("type", "").lower() == "magazine"]
    publishers = [n for n in base_nodes if n.get("type", "").lower() == "publisher"]
    
    if authors:
        author_name = authors[0].get("properties", {}).get("name") or authors[0].get("name") or authors[0].get("label") or "ä¸æ˜"
        lines.append(f"ã‚¯ã‚¨ãƒªã®ä½œå“ã®ä½œè€…: {author_name}")
    
    if magazines:
        mag_name = magazines[0].get("properties", {}).get("name") or magazines[0].get("name") or magazines[0].get("label") or "ä¸æ˜"
        lines.append(f"ã‚¯ã‚¨ãƒªãŒæ²è¼‰ã•ã‚ŒãŸé›‘èªŒ: {mag_name}")
    
    if publishers:
        pub_name = publishers[0].get("properties", {}).get("name") or publishers[0].get("name") or publishers[0].get("label") or "ä¸æ˜"
        lines.append(f"ã‚¯ã‚¨ãƒªãŒæ²è¼‰ã•ã‚ŒãŸé›‘èªŒã®å‡ºç‰ˆç¤¾: {pub_name}")
    
    # è‘—è€…ã®åˆ¥ä½œå“
    author_works_list = extended_info.get("author_works", [])
    if author_works_list:
        lines.append("")
        author_name = authors[0].get("properties", {}).get("name") or authors[0].get("name") if authors else "ä½œè€…"
        lines.append(f"### {author_name}ã®åˆ¥ä½œå“")
        work_titles_added = set()
        for aw in author_works_list:
            for node in aw.get("nodes", []):
                if node.get("type", "").lower() == "work":
                    title = get_work_title(node)
                    if title and title.lower() != work_title.lower() and title not in work_titles_added:
                        lines.append(f"- {title}")
                        work_titles_added.add(title)
        if not work_titles_added:
            lines.append("- ãªã—")
    
    # åŒé›‘èªŒã®åˆ¥ä½œå“
    magazine_works_list = extended_info.get("magazine_works", [])
    if magazine_works_list:
        lines.append("")
        lines.append("### åŒé›‘èªŒã®åˆ¥ä½œå“")
        work_titles_added = set()
        for mw in magazine_works_list:
            nodes_dict = {n.get("id") or n.get("elementId"): n for n in mw.get("nodes", [])}
            for node in mw.get("nodes", []):
                if node.get("type", "").lower() == "work":
                    title = get_work_title(node)
                    if title and title.lower() != work_title.lower() and title not in work_titles_added:
                        # ä½œè€…ã‚’æ¢ã™
                        work_author = "ä¸æ˜"
                        for edge in mw.get("edges", []) or mw.get("relationships", []) or []:
                            if edge.get("type") == "created" and edge.get("target") == (node.get("id") or node.get("elementId")):
                                author_node = nodes_dict.get(edge.get("source"))
                                if author_node:
                                    work_author = author_node.get("properties", {}).get("name") or author_node.get("name") or "ä¸æ˜"
                        mag_name = magazines[0].get("properties", {}).get("name") if magazines else "ä¸æ˜"
                        pub_name = publishers[0].get("properties", {}).get("name") if publishers else "ä¸æ˜"
                        lines.append(f"- {title}ï¼ˆä½œè€…: {work_author}ã€é›‘èªŒ: {mag_name}ã€å‡ºç‰ˆç¤¾: {pub_name}ï¼‰")
                        work_titles_added.add(title)
        if not work_titles_added:
            lines.append("- ãªã—")
    
    # åŒå‡ºç‰ˆç¤¾ã®ä»–èªŒã®ä½œå“
    other_mag_works = extended_info.get("other_magazines_works", [])
    if other_mag_works:
        lines.append("")
        lines.append("### åŒå‡ºç‰ˆç¤¾ã®ä»–èªŒã«æ²è¼‰ã•ã‚ŒãŸä½œå“")
        work_titles_added = set()
        for omw in other_mag_works:
            nodes_dict = {n.get("id") or n.get("elementId"): n for n in omw.get("nodes", [])}
            for node in omw.get("nodes", []):
                if node.get("type", "").lower() == "work":
                    title = get_work_title(node)
                    if title and title.lower() != work_title.lower() and title not in work_titles_added:
                        # ä½œè€…ã¨é›‘èªŒã‚’æ¢ã™
                        work_author = "ä¸æ˜"
                        work_mag = "ä¸æ˜"
                        for edge in omw.get("edges", []) or omw.get("relationships", []) or []:
                            node_id = node.get("id") or node.get("elementId")
                            if edge.get("type") == "created" and edge.get("target") == node_id:
                                author_node = nodes_dict.get(edge.get("source"))
                                if author_node:
                                    work_author = author_node.get("properties", {}).get("name") or author_node.get("name") or "ä¸æ˜"
                            if edge.get("type") == "published" and edge.get("target") == node_id:
                                mag_node = nodes_dict.get(edge.get("source"))
                                if mag_node:
                                    work_mag = mag_node.get("properties", {}).get("name") or mag_node.get("name") or "ä¸æ˜"
                        pub_name = publishers[0].get("properties", {}).get("name") if publishers else "ä¸æ˜"
                        lines.append(f"- {title}ï¼ˆä½œè€…: {work_author}ã€é›‘èªŒ: {work_mag}ã€å‡ºç‰ˆç¤¾: {pub_name}ï¼‰")
                        work_titles_added.add(title)
                        if len(work_titles_added) >= 5:
                            break
            if len(work_titles_added) >= 5:
                break
        if not work_titles_added:
            lines.append("- ãªã—")
    
    return "\n".join(lines)


def generate_graphrag_recommendation(
    user_input: str,
    context: str,
    token_callback: Optional[Callable[[str], None]] = None,
) -> str:
    """GraphRAGãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‰æ–‡ã‚’ç”Ÿæˆ"""
    rec_prompt = GraphRAGPrompts.get_recommendation_prompt()
    prompt_text = rec_prompt.format(user_query=user_input, context=context)
    
    body = {
        "text": prompt_text,
        "model": "gpt-4.1-nano",
        "temperature": 0.7,
        "max_tokens": 1000,
        "streaming": True,
    }
    
    full_text = ""
    try:
        r = request_with_retry(
            "POST",
            TEXT_GEN_ENDPOINT,
            json=body,
            headers=_auth_headers({"Content-Type": "application/json"}),
            timeout=180,
            stream=True,
        )
        with r:
            r.raise_for_status()
            buffer = ""
            for chunk in r.iter_content(chunk_size=None, decode_unicode=True):
                buffer += chunk
                while "\n\n" in buffer:
                    message, buffer = buffer.split("\n\n", 1)
                    if message.startswith("data: "):
                        line = message[6:].strip()
                        if not line:
                            continue
                        appended = ""
                        try:
                            if line.startswith("{") and line.endswith("}"):
                                data = json.loads(line)
                                if isinstance(data, dict) and "text" in data:
                                    appended = str(data["text"])
                            else:
                                appended = line
                        except Exception:
                            appended = line
                        if appended:
                            full_text += appended
                            if token_callback:
                                token_callback(appended)
    except Exception as e:
        logger.error("GraphRAG generation failed: %s", e)
        return full_text + f"\n[GraphRAGç”Ÿæˆã‚¨ãƒ©ãƒ¼] {e}"
    
    return full_text or "(ç”Ÿæˆçµæœãªã—)"


def run_graphrag_pipeline_new(
    user_input: str,
    token_callback: Optional[Callable[[str], None]] = None,
    selected_title: str | None = None,
) -> Dict[str, Any]:
    """
    æ–°ã—ã„GraphRAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆçµ±åˆAPIä½¿ç”¨ï¼‰
    1-6: ã‚°ãƒ©ãƒ•æ¤œç´¢ï¼ˆcascadeçµ±åˆAPIï¼‰
    7-8: é¡ä¼¼æ¤œç´¢ï¼ˆsimilarity/multiçµ±åˆAPIï¼‰- ã‚°ãƒ©ãƒ•æ¤œç´¢ã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
    11-14: æ‹¡å¼µã‚°ãƒ©ãƒ•æƒ…å ±å–å¾—ï¼ˆbatchçµ±åˆAPI + work-graphï¼‰
    15-16: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã¨ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‰ç”Ÿæˆ
    
    é¡ä¼¼æ¤œç´¢ã§å€™è£œãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã¯å€™è£œã‚’è¿”ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é¸æŠã•ã›ã‚‹
    """
    query = selected_title or user_input
    
    # 1-6: ã‚°ãƒ©ãƒ•æ¤œç´¢ï¼ˆçµ±åˆAPIï¼‰
    base_graph, search_mode = perform_graph_search(query)
    
    fuzzy_used = False
    similarity_candidates = []
    
    # ã‚°ãƒ©ãƒ•æ¤œç´¢ã§è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆã¯é¡ä¼¼æ¤œç´¢ï¼ˆãŸã ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ—¢ã«é¸æŠæ¸ˆã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
    if not base_graph.get("nodes") and selected_title is None:
        fuzzy_used = True
        # 7-8: ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼æ¤œç´¢ï¼ˆçµ±åˆAPIï¼‰
        similarity_result = search_vector_similarity_multi(query, limit=10, threshold=0.3)
        results = similarity_result.get("results", []) or []
        
        # å€™è£œãƒªã‚¹ãƒˆã‚’æ§‹ç¯‰
        for r in results:
            title = r.get("title_ja") or r.get("title_en") or ""
            score = r.get("similarity_score") or 0
            if title and title not in [c["title"] for c in similarity_candidates]:
                similarity_candidates.append({
                    "title": title,
                    "score": score,
                    "work_id": r.get("work_id"),
                })
        
        # å€™è£œãŒã‚ã‚‹å ´åˆã€é¸æŠã‚’å¾…ã¤ï¼ˆå€™è£œã‚’è¿”ã™ï¼‰
        if similarity_candidates:
            return {
                "extracted_title": query,
                "fuzzy_used": True,
                "fuzzy_best_title": similarity_candidates[0]["title"],
                "user_selected_candidate": False,
                "search_mode": "",
                "graph_summary": "",
                "graph_debug": "",
                "recommendation": "",
                "raw_graph": {},
                "similarity_candidates": similarity_candidates,
                "not_found": False,
                "awaiting_selection": True,  # ãƒ¦ãƒ¼ã‚¶ãƒ¼é¸æŠå¾…ã¡ãƒ•ãƒ©ã‚°
            }
    
    # ã‚°ãƒ©ãƒ•æ¤œç´¢ã§è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆï¼ˆé¡ä¼¼æ¤œç´¢å¾Œã‚‚å«ã‚€ï¼‰
    if not base_graph.get("nodes"):
        return {
            "extracted_title": query,
            "fuzzy_used": fuzzy_used,
            "fuzzy_best_title": similarity_candidates[0]["title"] if similarity_candidates else None,
            "user_selected_candidate": selected_title is not None,
            "search_mode": "",
            "graph_summary": "",
            "graph_debug": "",
            "recommendation": "",
            "raw_graph": {},
            "similarity_candidates": similarity_candidates,
            "not_found": True,  # æ¤œç´¢çµæœãªã—ãƒ•ãƒ©ã‚°
            "awaiting_selection": False,
        }
    
    # 11-14: æ‹¡å¼µã‚°ãƒ©ãƒ•æƒ…å ±å–å¾—ï¼ˆçµ±åˆAPIï¼‰
    extended_info = fetch_extended_graph_info(base_graph)
    
    # 15: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
    context = build_graph_context_from_extended(extended_info, query)
    
    # 16: ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‰ç”Ÿæˆ
    recommendation = generate_graphrag_recommendation(user_input, context, token_callback)
    
    return {
        "extracted_title": query,
        "fuzzy_used": fuzzy_used,
        "fuzzy_best_title": similarity_candidates[0]["title"] if similarity_candidates else None,
        "user_selected_candidate": selected_title is not None,
        "search_mode": search_mode,
        "graph_summary": context,
        "graph_debug": json.dumps(extended_info, ensure_ascii=False, indent=2)[:2000],
        "recommendation": recommendation,
        "raw_graph": base_graph,
        "similarity_candidates": similarity_candidates,
        "not_found": False,
        "awaiting_selection": False,
    }


def stream_generate(text, container, title):
    """APIã‹ã‚‰ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—ã—ã¦è¡¨ç¤º"""
    try:
        api_base = os.getenv("API_BASE", "http://localhost:8000")
        url = f"{api_base}/text-generation/generate"
        headers = _auth_headers({"Content-Type": "application/json"})
        data = {"text": text, "streaming": "true"}

        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å‡¦ç†
        # 502ç­‰ãŒå‡ºã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚ã€æ¥ç¶šç¢ºç«‹ã¾ã§ãƒªãƒˆãƒ©ã‚¤
        # on_retryã§UIã«èµ·å‹•å¾…ã¡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
        def on_retry(ctx: dict):
            wait = ctx.get("wait")
            status = ctx.get("status")
            if status in (502, 503, 504) or status is None:
                container.info(
                    f"ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰èµ·å‹•å¾…ã¡ä¸­... ãƒªãƒˆãƒ©ã‚¤{ctx.get('attempt')}å›ç›®ã€‚"
                    + (f" æ¬¡ã®è©¦è¡Œã¾ã§ç´„{wait:.1f}ç§’" if wait else "")
                )

        response = request_with_retry(
            "POST",
            url,
            json=data,
            headers=headers,
            stream=True,
            timeout=180,
            on_retry=on_retry,
        )
        response.raise_for_status()  # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯

        full_text = ""
        buffer = ""
        with container.container():
            st.subheader(title)
            text_placeholder = st.empty()

            for chunk in response.iter_content(chunk_size=None, decode_unicode=True):
                buffer += chunk
                while "\n\n" in buffer:
                    message, buffer = buffer.split("\n\n", 1)
                    if message.startswith("data: "):
                        line = message[len("data: ") :].strip()
                        if not line:
                            continue

                        # lineãŒJSONå½¢å¼ï¼ˆ"{...}"ï¼‰ã§ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                        if line.startswith("{") and line.endswith("}"):
                            try:
                                json_data = json.loads(line)
                                if isinstance(json_data, dict):
                                    if "text" in json_data:
                                        full_text += str(json_data["text"])
                                    elif "content" in json_data:
                                        full_text += str(json_data["content"])
                                    else:
                                        # ä»–ã®ã‚­ãƒ¼ã‚‚è€ƒæ…®
                                        full_text += " ".join(
                                            [str(v) for v in json_data.values() if isinstance(v, (str, int, float))]
                                        )
                                else:
                                    full_text += str(json_data)
                            except json.JSONDecodeError:
                                # JSONãƒ‡ã‚³ãƒ¼ãƒ‰ã«å¤±æ•—ã—ãŸå ´åˆã¯ã€æ–‡å­—åˆ—ã¨ã—ã¦ãã®ã¾ã¾è¿½åŠ 
                                full_text += line
                        else:
                            # JSONå½¢å¼ã§ãªã„å ´åˆã¯ã€ãã®ã¾ã¾ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦è¿½åŠ 
                            full_text += line

                        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§è¡¨ç¤ºã‚’æ›´æ–°
                        text_placeholder.markdown(full_text)
                        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿æŒã—ã¦å†æç”»æ™‚ã‚‚è¡¨ç¤ºã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
                        st.session_state["raw_llm_output"] = full_text
                        time.sleep(0.01)  # å°‘ã—é…å»¶ã‚’å…¥ã‚Œã¦è¡¨ç¤ºã‚’è¦‹ã‚„ã™ãã™ã‚‹
        # å®Œäº†ãƒ•ãƒ©ã‚°
        st.session_state["raw_llm_done"] = True
    except requests.exceptions.HTTPError as e:
        with container.container():
            st.subheader(title)
            st.error(f"APIå‘¼ã³å‡ºã—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {e.response.status_code}")
            st.text(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {e.response.text}")
        st.session_state["raw_llm_output"] = f"APIã‚¨ãƒ©ãƒ¼: {e.response.status_code}\n{e.response.text}"
        st.session_state["raw_llm_done"] = True

    except requests.exceptions.ConnectionError:
        with container.container():
            st.subheader(title)
            st.error("APIã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“ã€‚API_ServerãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.session_state["raw_llm_output"] = "APIã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
        st.session_state["raw_llm_done"] = True
    except Exception as e:
        with container.container():
            st.subheader(title)
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        st.session_state["raw_llm_output"] = f"ã‚¨ãƒ©ãƒ¼: {str(e)}"
        st.session_state["raw_llm_done"] = True


def main():
    st.title("ğŸ“š GraphRAGã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆãƒ‡ãƒ¢")
    st.markdown("åŒã˜ãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã—ã¦ç´ ã®LLMï¼ˆGraphRAGãªã—ï¼‰ã¨GraphRAGã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆã®çµæœã‚’æ¯”è¼ƒè¡¨ç¤ºã—ã¾ã™ã€‚")
    # å³ä¸‹ã«å°ã•ãªã€Œå‡ºå…¸ã€ãƒªãƒ³ã‚¯ï¼ˆãƒ•ãƒ­ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ï¼‰
    st.markdown(
        """
        <style>
        .floating-citation-link {
            position: fixed;
            right: 16px;
            bottom: 12px;
            background: rgba(255,255,255,0.85);
            backdrop-filter: blur(6px);
            border: 1px solid #e6e6e6;
            border-radius: 8px;
            padding: 4px 8px;
            font-size: 12px;
            z-index: 9999;
            box-shadow: 0 2px 6px rgba(0,0,0,0.08);
        }
        .floating-citation-link a {
            color: #4f46e5;
            text-decoration: none;
        }
        .floating-citation-link a:hover {
            text-decoration: underline;
        }
        </style>
        <div class="floating-citation-link">
            ğŸ”— <a href="/source_link" target="_self">å‡ºå…¸</a>
        </div>
        """,
        unsafe_allow_html=True,
    )

    # å…¥åŠ›æ¬„ + å·»æ•°ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆPCã§ã¯æ¨ªä¸¦ã³ 4:1 / ãƒ¢ãƒã‚¤ãƒ«ã§ã¯è‡ªå‹•ç¸¦ç©ã¿ï¼‰
    st.subheader("ğŸ”¤ æ¼«ç”»å…¥åŠ›ã¨ãƒ•ã‚£ãƒ«ã‚¿")
    col_title, col_vol = st.columns([4, 1], gap="small")
    with col_title:
        input_text = st.text_area(
            "ãŠã™ã™ã‚æ–‡ã‚’ç”Ÿæˆã—ãŸã„æ¼«ç”»åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚:",
            height=100,
            placeholder="ä¾‹: NARUTO",
        )
    with col_vol:
        min_vol = st.number_input(
            "nå·»ä»¥ä¸Šç™ºè¡Œ (â‰¤10)",
            min_value=1,
            max_value=10,
            value=5,
            step=1,
            help="æŒ‡å®šã—ãŸå·»æ•°ä»¥ä¸Šã®å˜è¡Œæœ¬ãŒç™ºè¡Œã•ã‚Œã¦ã„ã‚‹ä½œå“ã«é™å®šã—ã¾ã™",
        )

    # æ¯”è¼ƒç”¨ã«ç´ ã®LLMã‚’å®Ÿè¡Œã™ã‚‹ã‹ã®åˆ‡ã‚Šæ›¿ãˆ
    show_raw_llm = st.checkbox(
        "ç´ ã®LLMï¼ˆGraphRAGãªã—ï¼‰ã‚‚å®Ÿè¡Œã—ã¦æ¯”è¼ƒã™ã‚‹",
        value=True,
        help="ã‚ªãƒ•ã«ã™ã‚‹ã¨ç´ ã®LLMã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦GraphRAGã®ã¿å®Ÿè¡Œã—ã¾ã™",
    )

    # å³ã‚«ãƒ©ãƒ ã«GraphRAGã®çµæœã‚’æ›¸ãè¾¼ã‚€ãƒ˜ãƒ«ãƒ‘ãƒ¼
    def run_graphrag_into(
        right_container,
        status_text,
        progress_bar,
        user_text: str,
        min_volumes: int,
        selected_title: str | None = None,
    ):
        status_text.text("ğŸ”„ GraphRAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œä¸­...")
        progress_bar.progress(60)
        with right_container:
            st.subheader("ğŸ•¸ï¸ GraphRAGã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆ")
            with st.spinner("Graph / æ¨è–¦ç”Ÿæˆä¸­..."):
                try:
                    reco_placeholder = st.empty()
                    buffer = []

                    def on_token(t: str):
                        buffer.append(t)
                        if "\n" in t or len(buffer) % 5 == 0 or t.endswith(("ã€‚", "!", "?")):
                            reco_placeholder.markdown("".join(buffer))

                    result = run_graphrag_pipeline_new(
                        user_text,
                        token_callback=on_token,
                        selected_title=selected_title,
                    )
                    
                    # é¡ä¼¼æ¤œç´¢ã§å€™è£œãŒè¦‹ã¤ã‹ã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼é¸æŠå¾…ã¡ã®å ´åˆ
                    if result.get("awaiting_selection"):
                        reco_placeholder.empty()
                        st.session_state["fuzzy_candidates"] = result.get("similarity_candidates", [])
                        st.session_state["awaiting_candidate_selection"] = True
                        st.session_state["pending_user_input"] = user_text
                        st.session_state["pending_min_vol"] = min_volumes
                        st.rerun()
                        return
                    
                    # æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆ
                    if result.get("not_found"):
                        reco_placeholder.warning("æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚åˆ¥ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãŠè©¦ã—ãã ã•ã„ã€‚")
                    else:
                        reco_placeholder.markdown(result["recommendation"])
                        with st.expander("æŠ½å‡ºãƒ»æ¤œç´¢ãƒ¡ã‚¿æƒ…å ±"):
                            meta_info = {
                                "extracted_title": result.get("extracted_title"),
                                "search_mode": result.get("search_mode"),
                                "fuzzy_used": result.get("fuzzy_used"),
                                "fuzzy_best_title": result.get("fuzzy_best_title"),
                                "user_selected_candidate": result.get("user_selected_candidate"),
                                "node_count": len(result.get("raw_graph", {}).get("nodes", []) or []),
                                "relationship_count": len(result.get("raw_graph", {}).get("edges", []) or []),
                            }
                            st.write(meta_info)
                            st.caption("ã‚°ãƒ©ãƒ•ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ")
                            st.text(result.get("graph_summary"))
                except ValueError as e:
                    st.error(str(e))
                except Exception as e:  # noqa: BLE001
                    st.error(f"GraphRAGå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        progress_bar.progress(90)
        progress_bar.progress(100)
        status_text.text("âœ… ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        time.sleep(1)
        progress_bar.empty()
        status_text.empty()
        st.success("âœ… ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")

    # å€™è£œé¸æŠãƒ‘ãƒãƒ«ã‚’è¡¨ç¤ºã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼
    def render_candidate_selector_panel(right_container):
        cands = st.session_state.get("fuzzy_candidates", [])
        base_query = st.session_state.get("pending_user_input", "")
        with right_container:
            st.subheader("ğŸ” é¡ä¼¼ã™ã‚‹å€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
            st.write("æ­£ã—ã„ä½œå“ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚é¸æŠå¾Œã«ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™ã€‚")
            st.caption(f"æ¤œç´¢èª: {base_query}")
            st.caption(f"å€™è£œä»¶æ•°: {len(cands)} ä»¶")

            if not cands:
                st.info("å€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ¤œç´¢æ¡ä»¶ã‚’å¤‰ãˆã¦ãŠè©¦ã—ãã ã•ã„ã€‚")
                return

            # å€™è£œã‚’ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã§è¡¨ç¤º
            options = []
            for c in cands:
                score_percent = c.get("score", 0) * 100
                options.append(f"{c['title']} (é¡ä¼¼åº¦: {score_percent:.1f}%)")
            
            idx = st.radio(
                "å€™è£œ",
                options=range(len(options)),
                format_func=lambda i: options[i],
                index=0,
                key="cand_idx",
            )
            
            cols = st.columns([1, 1])
            with cols[0]:
                if st.button("ã“ã®ä½œå“ã§ç”Ÿæˆã™ã‚‹", type="primary"):
                    chosen = cands[idx]
                    st.session_state["chosen_title"] = chosen["title"]
                    st.session_state["awaiting_candidate_selection"] = False
                    st.session_state["start_generation"] = True
                    st.rerun()
            with cols[1]:
                if st.button("ã‚­ãƒ£ãƒ³ã‚»ãƒ«"):
                    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢
                    for k in ["fuzzy_candidates", "awaiting_candidate_selection", "pending_user_input", "pending_min_vol"]:
                        if k in st.session_state:
                            del st.session_state[k]
                    st.rerun()

    # é¸æŠå¾…ã¡ãªã‚‰ã€ç”ŸLLMçµæœã‚’å·¦ã«ä¿æŒè¡¨ç¤ºã—ã¤ã¤ã€å€™è£œé¸æŠãƒ‘ãƒãƒ«ã‚’å‡ºã™ï¼ˆGraphRAGã¯æœªå®Ÿè¡Œï¼‰
    if st.session_state.get("awaiting_candidate_selection"):
        st.markdown("---")
        st.subheader("ğŸ“Š ç”Ÿæˆçµæœã®æ¯”è¼ƒ")
        col1, col2 = st.columns(2)
        with col1.container():
            st.subheader("ğŸ’¬ ç´ ã®LLMï¼ˆGraphRAGãªã—ï¼‰")
            raw_out = st.session_state.get("raw_llm_output")
            if raw_out:
                st.markdown(raw_out)
            else:
                st.info("ç´ ã®LLMã®çµæœã¯ã“ã“ã«è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
        with col2.container():
            st.subheader("ğŸ•¸ï¸ GraphRAGã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆ")
            st.info("å€™è£œã‚’é¸æŠã™ã‚‹ã¨GraphRAGã®ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™ã€‚")
        st.markdown("---")
        render_candidate_selector_panel(col2.container())
        st.stop()

    # é¸æŠå¾Œã«è‡ªå‹•å®Ÿè¡Œ
    if st.session_state.get("start_generation"):
        st.markdown("---")
        st.subheader("ğŸ“Š ç”Ÿæˆçµæœã®æ¯”è¼ƒ")
        col1, col2 = st.columns(2)
        progress_bar = st.progress(0)
        status_text = st.empty()

        # å·¦ã«ä¿å­˜æ¸ˆã¿ã®ç´ ã®LLMçµæœã‚’è¡¨ç¤ºï¼ˆå†ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯ã—ãªã„ï¼‰
        with col1.container():
            st.subheader("ğŸ’¬ ç´ ã®LLMï¼ˆGraphRAGãªã—ï¼‰")
            raw_out = st.session_state.get("raw_llm_output")
            if raw_out:
                st.markdown(raw_out)
            else:
                st.info("ç´ ã®LLMã®çµæœã¯ã“ã“ã«è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")

        run_graphrag_into(
            col2.container(),
            status_text,
            progress_bar,
            st.session_state.get("pending_user_input", input_text),
            st.session_state.get("pending_min_vol", int(min_vol)),
            selected_title=st.session_state.get("chosen_title"),
        )
        # å¾Œç‰‡ä»˜ã‘
        for k in [
            "fuzzy_candidates",
            "awaiting_candidate_selection",
            "pending_user_input",
            "pending_min_vol",
            "chosen_title",
            "start_generation",
        ]:
            if k in st.session_state:
                del st.session_state[k]

    # å®Ÿè¡Œãƒœã‚¿ãƒ³æŠ¼ä¸‹æ™‚ã®å‡¦ç†ï¼ˆç´ ã®LLMâ†’GraphRAGï¼‰
    if st.button("ğŸš€ ç”Ÿæˆé–‹å§‹", type="primary", use_container_width=True):
        if not input_text.strip():
            st.warning("âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        else:
            try:
                # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼ˆæ¯”è¼ƒè¡¨ç¤ºï¼‰ã¨ç”Ÿæˆ
                st.markdown("---")
                st.subheader("ğŸ“Š ç”Ÿæˆçµæœã®æ¯”è¼ƒ")
                col1, col2 = st.columns(2)
                progress_bar = st.progress(0)
                status_text = st.empty()

                if show_raw_llm:
                    with col1.container():
                        prompt = get_standard_recommend_prompt(input_text)
                        stream_generate(prompt, col1, "ğŸ’¬ ç´ ã®LLMï¼ˆGraphRAGãªã—ï¼‰")

                # GraphRAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ
                run_graphrag_into(
                    col2.container(),
                    status_text,
                    progress_bar,
                    input_text,
                    int(min_vol),
                    selected_title=None,
                )
            except Exception as e:
                st.error(f"å‰å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    # APIã‚µãƒ¼ãƒãƒ¼ã®çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯
    st.markdown("---")
    st.subheader("ğŸ”§ ã‚µãƒ¼ãƒãƒ¼çŠ¶æ…‹")

    if st.button("ã‚µãƒ¼ãƒãƒ¼æ¥ç¶šç¢ºèª"):
        check_server_connection(os.getenv("API_BASE", "http://localhost:8000"))


def check_server_connection(api_base: str):
    try:
        response = request_with_retry("GET", f"{api_base}/health", headers=_auth_headers(), timeout=5)
        if response.status_code == 200:
            st.success("âœ… APIã‚µãƒ¼ãƒãƒ¼ã«æ­£å¸¸ã«æ¥ç¶šã§ãã¾ã™")
        else:
            st.warning(f"âš ï¸ ã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰ã®å¿œç­”ãŒç•°å¸¸ã§ã™ (ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status_code})")
    except requests.exceptions.ConnectionError:
        st.error("âŒ APIã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“ã€‚API_ServerãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    except Exception as e:
        st.error(f"âŒ æ¥ç¶šç¢ºèªä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")


def get_standard_recommend_prompt(user_query: str) -> str:
    prompt_template = StandardMangaPrompts.get_recommendation_prompt()
    return prompt_template.format(user_query=user_query)


if __name__ == "__main__":
    main()
