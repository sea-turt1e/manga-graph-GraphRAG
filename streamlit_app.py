import json
import logging
import os
import time
from copy import deepcopy

import requests
import streamlit as st
from dotenv import load_dotenv

from graphrag_service import extract_formal_title, fuzzy_search, run_graphrag_pipeline, strict_search
from prompts.manga_prompts import StandardMangaPrompts

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

st.set_page_config(page_title="GraphRAGã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆãƒ‡ãƒ¢", page_icon="ðŸ“š", layout="wide")
load_dotenv()


def stream_generate(text, container, title):
    """APIã‹ã‚‰ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—ã—ã¦è¡¨ç¤º"""
    try:
        api_base = os.getenv("API_BASE", "http://localhost:8000")
        url = f"{api_base}/text-generation/generate"
        headers = {"Content-Type": "application/json"}
        data = {"text": text, "streaming": "true"}

        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å‡¦ç†
        response = requests.post(url, json=data, headers=headers, stream=True)
        response.raise_for_status()  # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯

        full_text = ""
        buffer = ""
        with container.container():
            st.subheader(title)
            text_placeholder = st.empty()

            for chunk in response.iter_content(chunk_size=None, decode_unicode=True):
                buffer += chunk
                while "\n\n" in buffer:
                    message, buffer = buffer.split("\n\n", 1)
                    if message.startswith("data: "):
                        line = message[len("data: ") :].strip()
                        if not line:
                            continue

                        # lineãŒJSONå½¢å¼ï¼ˆ"{...}"ï¼‰ã§ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                        if line.startswith("{") and line.endswith("}"):
                            try:
                                json_data = json.loads(line)
                                if isinstance(json_data, dict):
                                    if "text" in json_data:
                                        full_text += str(json_data["text"])
                                    elif "content" in json_data:
                                        full_text += str(json_data["content"])
                                    else:
                                        # ä»–ã®ã‚­ãƒ¼ã‚‚è€ƒæ…®
                                        full_text += " ".join(
                                            [str(v) for v in json_data.values() if isinstance(v, (str, int, float))]
                                        )
                                else:
                                    full_text += str(json_data)
                            except json.JSONDecodeError:
                                # JSONãƒ‡ã‚³ãƒ¼ãƒ‰ã«å¤±æ•—ã—ãŸå ´åˆã¯ã€æ–‡å­—åˆ—ã¨ã—ã¦ãã®ã¾ã¾è¿½åŠ 
                                full_text += line
                        else:
                            # JSONå½¢å¼ã§ãªã„å ´åˆã¯ã€ãã®ã¾ã¾ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦è¿½åŠ 
                            full_text += line

                        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§è¡¨ç¤ºã‚’æ›´æ–°
                        text_placeholder.markdown(full_text)
                        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿æŒã—ã¦å†æç”»æ™‚ã‚‚è¡¨ç¤ºã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
                        st.session_state["raw_llm_output"] = full_text
                        time.sleep(0.01)  # å°‘ã—é…å»¶ã‚’å…¥ã‚Œã¦è¡¨ç¤ºã‚’è¦‹ã‚„ã™ãã™ã‚‹
        # å®Œäº†ãƒ•ãƒ©ã‚°
        st.session_state["raw_llm_done"] = True
    except requests.exceptions.HTTPError as e:
        with container.container():
            st.subheader(title)
            st.error(f"APIå‘¼ã³å‡ºã—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {e.response.status_code}")
            st.text(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {e.response.text}")
        st.session_state["raw_llm_output"] = f"APIã‚¨ãƒ©ãƒ¼: {e.response.status_code}\n{e.response.text}"
        st.session_state["raw_llm_done"] = True

    except requests.exceptions.ConnectionError:
        with container.container():
            st.subheader(title)
            st.error("APIã‚µãƒ¼ãƒãƒ¼ã«æŽ¥ç¶šã§ãã¾ã›ã‚“ã€‚API_ServerãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.session_state["raw_llm_output"] = "APIã‚µãƒ¼ãƒãƒ¼ã«æŽ¥ç¶šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
        st.session_state["raw_llm_done"] = True
    except Exception as e:
        with container.container():
            st.subheader(title)
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        st.session_state["raw_llm_output"] = f"ã‚¨ãƒ©ãƒ¼: {str(e)}"
        st.session_state["raw_llm_done"] = True


def main():
    st.title("ðŸ“š GraphRAGã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆãƒ‡ãƒ¢")
    st.markdown("åŒã˜ãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã—ã¦ç´ ã®LLMï¼ˆGraphRAGãªã—ï¼‰ã¨GraphRAGã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆã®çµæžœã‚’æ¯”è¼ƒè¡¨ç¤ºã—ã¾ã™ã€‚")

    # å…¥åŠ›æ¬„ + å·»æ•°ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆPCã§ã¯æ¨ªä¸¦ã³ 4:1 / ãƒ¢ãƒã‚¤ãƒ«ã§ã¯è‡ªå‹•ç¸¦ç©ã¿ï¼‰
    st.subheader("ðŸ”¤ æ¼«ç”»å…¥åŠ›ã¨ãƒ•ã‚£ãƒ«ã‚¿")
    col_title, col_vol = st.columns([4, 1], gap="small")
    with col_title:
        input_text = st.text_area(
            "ãŠã™ã™ã‚æ–‡ã‚’ç”Ÿæˆã—ãŸã„æ¼«ç”»åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚:",
            height=100,
            placeholder="ä¾‹: NARUTO",
        )
    with col_vol:
        min_vol = st.number_input(
            "nå·»ä»¥ä¸Šç™ºè¡Œ (â‰¤10)",
            min_value=1,
            max_value=10,
            value=5,
            step=1,
            help="æŒ‡å®šã—ãŸå·»æ•°ä»¥ä¸Šã®å˜è¡Œæœ¬ãŒç™ºè¡Œã•ã‚Œã¦ã„ã‚‹ä½œå“ã«é™å®šã—ã¾ã™",
        )

    # æ¯”è¼ƒç”¨ã«ç´ ã®LLMã‚’å®Ÿè¡Œã™ã‚‹ã‹ã®åˆ‡ã‚Šæ›¿ãˆ
    show_raw_llm = st.checkbox(
        "ç´ ã®LLMï¼ˆGraphRAGãªã—ï¼‰ã‚‚å®Ÿè¡Œã—ã¦æ¯”è¼ƒã™ã‚‹",
        value=True,
        help="ã‚ªãƒ•ã«ã™ã‚‹ã¨ç´ ã®LLMã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦GraphRAGã®ã¿å®Ÿè¡Œã—ã¾ã™",
    )

    # å³ã‚«ãƒ©ãƒ ã«GraphRAGã®çµæžœã‚’æ›¸ãè¾¼ã‚€ãƒ˜ãƒ«ãƒ‘ãƒ¼
    def run_graphrag_into(
        right_container,
        status_text,
        progress_bar,
        user_text: str,
        min_volumes: int,
        selected_title: str | None = None,
    ):
        status_text.text("ðŸ”„ GraphRAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œä¸­...")
        progress_bar.progress(60)
        with right_container:
            st.subheader("ðŸ•¸ï¸ GraphRAGã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆ")
            with st.spinner("Graph / æŽ¨è–¦ç”Ÿæˆä¸­..."):
                try:
                    reco_placeholder = st.empty()
                    buffer = []

                    def on_token(t: str):
                        buffer.append(t)
                        if "\n" in t or len(buffer) % 5 == 0 or t.endswith(("ã€‚", "!", "?")):
                            reco_placeholder.markdown("".join(buffer))

                    result = run_graphrag_pipeline(
                        user_text,
                        token_callback=on_token,
                        min_total_volumes=int(min_volumes),
                        selected_title=selected_title,
                    )
                    reco_placeholder.markdown(result["recommendation"])
                    with st.expander("æŠ½å‡ºãƒ»æ¤œç´¢ãƒ¡ã‚¿æƒ…å ±"):
                        st.write(
                            {
                                "extracted_title": result.get("extracted_title"),
                                "fuzzy_used": result.get("fuzzy_used"),
                                "fuzzy_best_title": result.get("fuzzy_best_title"),
                                "user_selected_candidate": result.get("user_selected_candidate"),
                                "node_count": result.get("raw_graph", {}).get("node_count"),
                                "relationship_count": result.get("raw_graph", {}).get("relationship_count"),
                            }
                        )
                        st.text(result.get("graph_summary"))
                except ValueError as e:
                    st.error(str(e))
                except Exception as e:  # noqa: BLE001
                    st.error(f"GraphRAGå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        progress_bar.progress(90)
        progress_bar.progress(100)
        status_text.text("âœ… ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        time.sleep(1)
        progress_bar.empty()
        status_text.empty()
        st.success("âœ… ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")

    # ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ï¼ˆãƒšãƒ¼ã‚¸å†…ï¼‰å€™è£œé¸æŠžãƒ‘ãƒãƒ«
    def render_candidate_selector_panel(right_container):  # uses session_state
        cands = st.session_state.get("fuzzy_candidates", [])
        base_query = st.session_state.get("dialog_extracted_title") or st.session_state.get("pending_user_input")
        with right_container:
            st.subheader("ðŸ”Ž å€™è£œãŒè¤‡æ•°è¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
            st.write("æ­£ã—ã„ä½œå“ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚é¸æŠžå¾Œã«ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™ã€‚")
            st.caption(f"æ¤œç´¢èªž: {base_query}")
            st.caption(f"å€™è£œä»¶æ•°: {len(cands)} ä»¶")

            if not cands:
                st.info("å€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ¤œç´¢æ¡ä»¶ã‚’å¤‰ãˆã¦ãŠè©¦ã—ãã ã•ã„ã€‚")
                return

            options = [c["display"] for c in cands]
            idx = st.radio(
                "å€™è£œ",
                options=range(len(options)),
                format_func=lambda i: options[i],
                index=0,
                key="cand_idx",
            )
            cols = st.columns([1, 1])
            with cols[0]:
                if st.button("ã“ã®ä½œå“ã§ç”Ÿæˆã™ã‚‹", type="primary"):
                    chosen = cands[idx]
                    st.session_state["chosen_title"] = chosen["title"]
                    st.session_state["awaiting_candidate_selection"] = False
                    st.session_state["start_generation"] = True
                    st.rerun()
            with cols[1]:
                if st.button("ä¸Šä½å€™è£œã§ç”Ÿæˆ"):
                    # ä¸Šä½å€™è£œã¾ãŸã¯æŠ½å‡ºã‚¿ã‚¤ãƒˆãƒ«ã§ç¶šè¡Œ
                    fallback = cands[0]["title"] if cands else (st.session_state.get("dialog_extracted_title") or "")
                    st.session_state["chosen_title"] = fallback
                    st.session_state["awaiting_candidate_selection"] = False
                    st.session_state["start_generation"] = True
                    st.rerun()

    # æ—§ãƒ•ãƒ©ã‚°ï¼ˆãƒ¢ãƒ¼ãƒ€ãƒ«ç”¨ï¼‰ãŒæ®‹ã£ã¦ã„ã‚Œã°æ–°ãƒ•ãƒ©ã‚°ã«ç§»è¡Œ
    if st.session_state.get("open_candidate_dialog"):
        st.session_state["awaiting_candidate_selection"] = True
        del st.session_state["open_candidate_dialog"]

    # é¸æŠžå¾…ã¡ãªã‚‰ã€ç”ŸLLMçµæžœã‚’å·¦ã«ä¿æŒè¡¨ç¤ºã—ã¤ã¤ã€å€™è£œé¸æŠžãƒ‘ãƒãƒ«ã‚’å‡ºã™ï¼ˆGraphRAGã¯æœªå®Ÿè¡Œï¼‰
    if st.session_state.get("awaiting_candidate_selection"):
        st.markdown("---")
        st.subheader("ðŸ“Š ç”Ÿæˆçµæžœã®æ¯”è¼ƒ")
        col1, col2 = st.columns(2)
        with col1.container():
            st.subheader("ðŸ’¬ ç´ ã®LLMï¼ˆGraphRAGãªã—ï¼‰")
            raw_out = st.session_state.get("raw_llm_output")
            if raw_out:
                st.markdown(raw_out)
            else:
                st.info("ç´ ã®LLMã®çµæžœã¯ã“ã“ã«è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
        with col2.container():
            st.subheader("ðŸ•¸ï¸ GraphRAGã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆ")
            st.info("å€™è£œã‚’é¸æŠžã™ã‚‹ã¨GraphRAGã®ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™ã€‚")
        st.markdown("---")
        render_candidate_selector_panel(col2.container())
        st.stop()

    # å®Ÿè¡Œãƒœã‚¿ãƒ³æŠ¼ä¸‹æ™‚ã®å‡¦ç†ï¼ˆã¾ãšç´ ã®LLMâ†’ãã®å¾Œã«åŽ³æ ¼/æŠ½å‡º/ã‚ã„ã¾ã„â†’å¿…è¦ãªã‚‰å€™è£œé¸æŠžâ†’GraphRAGï¼‰
    if st.button("ðŸš€ ç”Ÿæˆé–‹å§‹", type="primary", use_container_width=True):
        if not input_text.strip():
            st.warning("âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        else:
            try:
                # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼ˆæ¯”è¼ƒè¡¨ç¤ºï¼‰ã¨ç”Ÿæˆ
                st.markdown("---")
                st.subheader("ðŸ“Š ç”Ÿæˆçµæžœã®æ¯”è¼ƒ")
                col1, col2 = st.columns(2)
                progress_bar = st.progress(0)
                status_text = st.empty()

                if show_raw_llm:
                    with col1.container():
                        prompt = get_standard_recommend_prompt(input_text)
                        stream_generate(prompt, col1, "ðŸ’¬ ç´ ã®LLMï¼ˆGraphRAGãªã—ï¼‰")

                # æ›–æ˜§æ€§è§£æ¶ˆï¼ˆå€™è£œé¸æŠžï¼‰ã‚’å®Œäº†ã•ã›ã‚‹ã€‚è§£æ±ºå¾Œã«ç”Ÿæˆã‚’é–‹å§‹ã™ã‚‹ã€‚
                # ã‚¹ãƒ”ãƒŠãƒ¼ã¨çµæžœUIã¯å³ã‚«ãƒ©ãƒ ã«è¡¨ç¤º
                with col2.container():
                    with st.spinner("ã‚°ãƒ©ãƒ•ã‹ã‚‰æ¼«ç”»åã‚’æ¤œç´¢ä¸­..."):
                        # 1) åŽ³æ ¼æ¤œç´¢ï¼ˆå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆï¼‰
                        strict_res = strict_search(input_text, min_total_volumes=int(min_vol))

                        selected_title_for_run: str | None = None
                        if strict_res.get("nodes"):
                            selected_title_for_run = None  # å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã§ãã®ã¾ã¾å®Ÿè¡Œ
                        else:
                            # 2) ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º â†’ åŽ³æ ¼
                            extracted = extract_formal_title(input_text)
                            if not extracted:
                                extracted = deepcopy(input_text)
                            strict2 = strict_search(extracted, min_total_volumes=int(min_vol))
                            if strict2.get("nodes"):
                                selected_title_for_run = extracted
                            else:
                                # 3) ã‚ã„ã¾ã„æ¤œç´¢
                                fz = fuzzy_search(extracted)
                                # ã•ã¾ã–ã¾ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢çŠ¶ã«å¯¾å¿œ
                                raw_candidates = fz.get("results") or fz.get("nodes") or []
                                # nodesé…åˆ—ã®å ´åˆã¯workã ã‘ã«çµžã‚‹
                                if (
                                    raw_candidates
                                    and isinstance(raw_candidates[0], dict)
                                    and "type" in raw_candidates[0]
                                ):
                                    raw_candidates = [n for n in raw_candidates if n.get("type") == "work"]

                                processed = []
                                for c in raw_candidates:
                                    title = None
                                    score = None
                                    if isinstance(c, dict):
                                        props = c.get("properties") or {}
                                        # ãƒ•ãƒ©ãƒƒãƒˆå½¢å¼ or propertieså½¢å¼ åŒæ–¹å¯¾å¿œ
                                        title = (
                                            props.get("title")
                                            or c.get("title")
                                            or props.get("name")
                                            or c.get("name")
                                            or props.get("work_title")
                                        )
                                        score = (
                                            props.get("similarity_score") or c.get("similarity_score") or c.get("score")
                                        )
                                    elif isinstance(c, str):
                                        title = c
                                    if title:
                                        disp = f"{title}" if score is None else f"{title} (score: {score:.3f})"
                                        processed.append({"title": title, "score": score, "display": disp})

                    # æ›–æ˜§æ€§ã®çµæžœã«å¿œã˜ã¦åˆ†å²
                    if "processed" in locals() and len(processed) > 1:
                        # 2ä»¶ä»¥ä¸Š â†’ ãƒšãƒ¼ã‚¸å†…ãƒ‘ãƒãƒ«ã§é¸æŠžã€é¸æŠžå¾Œã«ç”Ÿæˆé–‹å§‹
                        st.session_state["fuzzy_candidates"] = processed
                        st.session_state["dialog_extracted_title"] = extracted
                        st.session_state["awaiting_candidate_selection"] = True
                        st.session_state["pending_user_input"] = input_text
                        st.session_state["pending_min_vol"] = int(min_vol)
                        st.session_state["pending_show_raw_llm"] = bool(show_raw_llm)
                        # ç¾åœ¨ã®ãƒ©ãƒ³ã§å³ã‚«ãƒ©ãƒ ã«ãƒ‘ãƒãƒ«è¡¨ç¤ºã¸ç§»è¡Œ
                        st.markdown("---")
                        render_candidate_selector_panel(col2.container())
                        st.stop()
                    else:
                        # å€™è£œ0/1ä»¶ â†’ ãã®ã¾ã¾ç”Ÿæˆé–‹å§‹
                        auto_title = None
                        if "processed" in locals():
                            auto_title = processed[0]["title"] if processed else extracted
                        final_selected_title = (
                            selected_title_for_run if selected_title_for_run is not None else auto_title
                        )

                    run_graphrag_into(
                        col2.container(),
                        status_text,
                        progress_bar,
                        input_text,
                        int(min_vol),
                        selected_title=final_selected_title,
                    )
            except Exception as e:
                st.error(f"å‰å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    # é¸æŠžå¾Œã«è‡ªå‹•å®Ÿè¡Œï¼ˆå·¦ã«ç´ ã®LLMçµæžœã‚’å†æŽ²ï¼‰
    if st.session_state.get("start_generation"):
        # é¸æŠžå¾Œã¯æ¯”è¼ƒè¡¨ç¤ºã‚’å†æ§‹ç¯‰ã—ã¦ç”Ÿæˆ
        st.markdown("---")
        st.subheader("ðŸ“Š ç”Ÿæˆçµæžœã®æ¯”è¼ƒ")
        col1, col2 = st.columns(2)
        progress_bar = st.progress(0)
        status_text = st.empty()

        # å·¦ã«ä¿å­˜æ¸ˆã¿ã®ç´ ã®LLMçµæžœã‚’è¡¨ç¤ºï¼ˆå†ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯ã—ãªã„ï¼‰
        with col1.container():
            st.subheader("ðŸ’¬ ç´ ã®LLMï¼ˆGraphRAGãªã—ï¼‰")
            raw_out = st.session_state.get("raw_llm_output")
            if raw_out:
                st.markdown(raw_out)
            else:
                st.info("ç´ ã®LLMã®çµæžœã¯ã“ã“ã«è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")

        run_graphrag_into(
            col2.container(),
            status_text,
            progress_bar,
            st.session_state.get("pending_user_input", input_text),
            st.session_state.get("pending_min_vol", int(min_vol)),
            selected_title=st.session_state.get("chosen_title"),
        )
        # å¾Œç‰‡ä»˜ã‘ï¼ˆãƒ¢ãƒ¼ãƒ€ãƒ«ã‚’é–‰ã˜ãŸã¾ã¾ã«ï¼‰
        for k in [
            "fuzzy_candidates",
            "dialog_extracted_title",
            "awaiting_candidate_selection",
            "pending_user_input",
            "pending_min_vol",
            "pending_show_raw_llm",
            "chosen_title",
            "start_generation",
        ]:
            if k in st.session_state:
                del st.session_state[k]

    # APIã‚µãƒ¼ãƒãƒ¼ã®çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯
    st.markdown("---")
    st.subheader("ðŸ”§ ã‚µãƒ¼ãƒãƒ¼çŠ¶æ…‹")

    if st.button("ã‚µãƒ¼ãƒãƒ¼æŽ¥ç¶šç¢ºèª"):
        check_server_connection(os.getenv("API_BASE", "http://localhost:8000"))


def check_server_connection(api_base: str):
    try:
        response = requests.get(f"{api_base}/health", timeout=5)
        if response.status_code == 200:
            st.success("âœ… APIã‚µãƒ¼ãƒãƒ¼ã«æ­£å¸¸ã«æŽ¥ç¶šã§ãã¾ã™")
        else:
            st.warning(f"âš ï¸ ã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰ã®å¿œç­”ãŒç•°å¸¸ã§ã™ (ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status_code})")
    except requests.exceptions.ConnectionError:
        st.error("âŒ APIã‚µãƒ¼ãƒãƒ¼ã«æŽ¥ç¶šã§ãã¾ã›ã‚“ã€‚API_ServerãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    except Exception as e:
        st.error(f"âŒ æŽ¥ç¶šç¢ºèªä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")


def get_standard_recommend_prompt(user_query: str) -> str:
    prompt_template = StandardMangaPrompts.get_recommendation_prompt()
    return prompt_template.format(user_query=user_query)


if __name__ == "__main__":
    main()
