import json
import logging
import os
import time
from typing import Any, Callable, Dict, List, Optional

import requests
import streamlit as st
from dotenv import load_dotenv

from prompts.manga_prompts import GraphRAGPrompts, StandardMangaPrompts
from retry_utils import request_with_retry

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

st.set_page_config(
    page_title="GraphRAGã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆãƒ‡ãƒ¢", page_icon="ğŸ“š", layout="wide", initial_sidebar_state="collapsed"
)
load_dotenv()

# Optional API key for backend
BACKEND_API_KEY = os.getenv("BACKEND_API_KEY", "").strip()
API_BASE = os.getenv("API_BASE", "http://localhost:8000")

# API Endpoints
GRAPH_SEARCH_ENDPOINT = f"{API_BASE}/api/v1/manga-anime-neo4j/graph"
VECTOR_SIMILARITY_ENDPOINT = f"{API_BASE}/api/v1/manga-anime-neo4j/vector/similarity"
AUTHOR_WORKS_ENDPOINT = f"{API_BASE}/api/v1/manga-anime-neo4j/author"
MAGAZINE_WORKS_ENDPOINT = f"{API_BASE}/api/v1/manga-anime-neo4j/magazine"
PUBLISHER_MAGAZINES_ENDPOINT = f"{API_BASE}/api/v1/manga-anime-neo4j/publisher"
MAGAZINES_WORK_GRAPH_ENDPOINT = f"{API_BASE}/api/v1/manga-anime-neo4j/magazines/work-graph"
TEXT_GEN_ENDPOINT = f"{API_BASE}/text-generation/generate"


def _auth_headers(extra: dict | None = None) -> dict:
    headers: dict = {}
    if BACKEND_API_KEY:
        headers["Authorization"] = f"Bearer {BACKEND_API_KEY}"
    headers["X-API-Key"] = BACKEND_API_KEY
    if extra:
        headers.update(extra)
    return headers


# =============================================================================
# Backend API Functions for GraphRAG
# =============================================================================


def search_graph(query: str, lang: str = "japanese", mode: str = "simple", limit: int = 3) -> Dict[str, Any]:
    """ã‚°ãƒ©ãƒ•æ¤œç´¢APIå‘¼ã³å‡ºã—"""
    params = {
        "q": query,
        "lang": lang,
        "mode": mode,
        "limit": limit,
        "include_hentai": False,
    }
    try:
        r = request_with_retry(
            "GET",
            GRAPH_SEARCH_ENDPOINT,
            params=params,
            headers=_auth_headers(),
            timeout=60,
        )
        r.raise_for_status()
        return r.json()
    except Exception as e:
        logger.warning("Graph search error: %s", e)
        return {}


def search_vector_similarity(query: str, embedding_type: str = "title_en") -> Dict[str, Any]:
    """ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼æ¤œç´¢APIå‘¼ã³å‡ºã—"""
    body = {
        "query": query,
        "embedding_type": embedding_type,
        "embedding_dims": 256,
        "limit": 10,
        "threshold": 0.3,
        "include_hentai": False,
    }
    try:
        r = request_with_retry(
            "POST",
            VECTOR_SIMILARITY_ENDPOINT,
            json=body,
            headers=_auth_headers({"Content-Type": "application/json"}),
            timeout=60,
        )
        r.raise_for_status()
        return r.json()
    except Exception as e:
        logger.warning("Vector similarity search error: %s", e)
        return {}


def get_author_works(author_node_id: str, limit: int = 5) -> Dict[str, Any]:
    """è‘—è€…ã®ä»–ä½œå“ã‚’å–å¾—"""
    url = f"{AUTHOR_WORKS_ENDPOINT}/{author_node_id}/works"
    params = {"limit": limit, "include_hentai": False}
    try:
        r = request_with_retry("GET", url, params=params, headers=_auth_headers(), timeout=60)
        r.raise_for_status()
        return r.json()
    except Exception as e:
        logger.warning("Author works error: %s", e)
        return {}


def get_magazine_works(magazine_node_id: str, limit: int = 5, reference_work_id: str | None = None) -> Dict[str, Any]:
    """é›‘èªŒã®ä»–ä½œå“ã‚’å–å¾—"""
    url = f"{MAGAZINE_WORKS_ENDPOINT}/{magazine_node_id}/works"
    params = {"limit": limit, "include_hentai": False}
    if reference_work_id:
        params["reference_work_id"] = reference_work_id
    try:
        r = request_with_retry("GET", url, params=params, headers=_auth_headers(), timeout=60)
        r.raise_for_status()
        return r.json()
    except Exception as e:
        logger.warning("Magazine works error: %s", e)
        return {}


def get_publisher_magazines(publisher_node_id: str, limit: int = 3, exclude_magazine_id: str | None = None) -> Dict[str, Any]:
    """å‡ºç‰ˆç¤¾ã®ä»–é›‘èªŒã‚’å–å¾—"""
    url = f"{PUBLISHER_MAGAZINES_ENDPOINT}/{publisher_node_id}/magazines"
    params = {"limit": limit, "include_hentai": False}
    if exclude_magazine_id:
        params["exclude_magazine_id"] = exclude_magazine_id
    try:
        r = request_with_retry("GET", url, params=params, headers=_auth_headers(), timeout=60)
        r.raise_for_status()
        return r.json()
    except Exception as e:
        logger.warning("Publisher magazines error: %s", e)
        return {}


def get_magazines_work_graph(magazine_ids: List[str], work_limit: int = 3, reference_work_id: str | None = None) -> Dict[str, Any]:
    """è¤‡æ•°é›‘èªŒã®ä½œå“ã‚°ãƒ©ãƒ•ã‚’å–å¾—"""
    body = {
        "magazine_ids": magazine_ids,
        "work_limit": work_limit,
        "include_hentai": False,
    }
    if reference_work_id:
        body["reference_work_id"] = reference_work_id
    try:
        r = request_with_retry(
            "POST",
            MAGAZINES_WORK_GRAPH_ENDPOINT,
            json=body,
            headers=_auth_headers({"Content-Type": "application/json"}),
            timeout=60,
        )
        r.raise_for_status()
        return r.json()
    except Exception as e:
        logger.warning("Magazines work graph error: %s", e)
        return {}


def extract_ids_from_graph(graph: Dict[str, Any]) -> Dict[str, Any]:
    """ã‚°ãƒ©ãƒ•ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒãƒ¼ãƒ‰IDã‚’æŠ½å‡º"""
    result = {
        "work_id": None,
        "work_title": None,
        "author_ids": [],
        "magazine_ids": [],
        "publisher_ids": [],
    }
    
    nodes = graph.get("nodes", []) or []
    edges = graph.get("edges", []) or graph.get("relationships", []) or []
    
    # ãƒãƒ¼ãƒ‰ã‚’ã‚¿ã‚¤ãƒ—åˆ¥ã«åˆ†é¡
    for node in nodes:
        node_type = node.get("type", "").lower()
        node_id = node.get("id") or node.get("elementId")
        
        if node_type == "work":
            if result["work_id"] is None:
                result["work_id"] = node_id
                result["work_title"] = node.get("properties", {}).get("title") or node.get("title") or node.get("label")
        elif node_type == "author":
            if node_id and node_id not in result["author_ids"]:
                result["author_ids"].append(node_id)
        elif node_type == "magazine":
            if node_id and node_id not in result["magazine_ids"]:
                result["magazine_ids"].append(node_id)
        elif node_type == "publisher":
            if node_id and node_id not in result["publisher_ids"]:
                result["publisher_ids"].append(node_id)
    
    return result


def perform_graph_search(query: str) -> tuple[Dict[str, Any], str]:
    """
    ã‚°ãƒ©ãƒ•æ¤œç´¢ã‚’æ®µéšçš„ã«å®Ÿè¡Œ
    1. japanese/simple -> 2. japanese/fulltext -> 3. japanese/ranked
    4. english/simple -> 5. english/fulltext -> 6. english/ranked
    
    Returns: (graph_response, search_mode_used)
    """
    search_modes = [
        ("japanese", "simple"),
        ("japanese", "fulltext"),
        ("japanese", "ranked"),
        ("english", "simple"),
        ("english", "fulltext"),
        ("english", "ranked"),
    ]
    
    for lang, mode in search_modes:
        result = search_graph(query, lang=lang, mode=mode, limit=3)
        nodes = result.get("nodes", []) or []
        if nodes:
            logger.info(f"Graph search found results with lang={lang}, mode={mode}")
            return result, f"{lang}/{mode}"
    
    return {}, ""


def perform_vector_similarity_search(query: str) -> List[Dict[str, Any]]:
    """
    ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼æ¤œç´¢ã‚’å®Ÿè¡Œ
    7. title_en -> 8. title_ja
    
    Returns: å€™è£œãƒªã‚¹ãƒˆ
    """
    candidates = []
    
    # 7. title_en ã§æ¤œç´¢
    result_en = search_vector_similarity(query, embedding_type="title_en")
    results_en = result_en.get("results", []) or result_en.get("nodes", []) or []
    for r in results_en:
        title = r.get("title") or r.get("properties", {}).get("title") or ""
        score = r.get("similarity_score") or r.get("score") or 0
        if title and title not in [c["title"] for c in candidates]:
            candidates.append({"title": title, "score": score, "source": "title_en"})
    
    # 8. title_ja ã§æ¤œç´¢
    result_ja = search_vector_similarity(query, embedding_type="title_ja")
    results_ja = result_ja.get("results", []) or result_ja.get("nodes", []) or []
    for r in results_ja:
        title = r.get("title") or r.get("properties", {}).get("title") or ""
        score = r.get("similarity_score") or r.get("score") or 0
        if title and title not in [c["title"] for c in candidates]:
            candidates.append({"title": title, "score": score, "source": "title_ja"})
    
    # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
    candidates.sort(key=lambda x: x.get("score", 0), reverse=True)
    return candidates


def fetch_extended_graph_info(base_graph: Dict[str, Any]) -> Dict[str, Any]:
    """
    11-14: è¿½åŠ ã®ã‚°ãƒ©ãƒ•æƒ…å ±ã‚’å–å¾—
    11. è‘—è€…ã®ä»–ä½œå“
    12. é›‘èªŒã®ä»–ä½œå“
    13. å‡ºç‰ˆç¤¾ã®ä»–é›‘èªŒ
    14. ä»–é›‘èªŒã®ä½œå“ã‚°ãƒ©ãƒ•
    """
    ids = extract_ids_from_graph(base_graph)
    
    extended_info = {
        "base_graph": base_graph,
        "author_works": [],
        "magazine_works": [],
        "publisher_magazines": [],
        "other_magazines_works": [],
        "extracted_ids": ids,
    }
    
    work_id = ids.get("work_id")
    
    # 11. è‘—è€…ã®ä»–ä½œå“
    for author_id in ids.get("author_ids", [])[:2]:  # æœ€å¤§2è‘—è€…
        author_works = get_author_works(author_id, limit=5)
        if author_works:
            extended_info["author_works"].append(author_works)
    
    # 12. é›‘èªŒã®ä»–ä½œå“
    for magazine_id in ids.get("magazine_ids", [])[:2]:  # æœ€å¤§2é›‘èªŒ
        magazine_works = get_magazine_works(magazine_id, limit=5, reference_work_id=work_id)
        if magazine_works:
            extended_info["magazine_works"].append(magazine_works)
    
    # 13. å‡ºç‰ˆç¤¾ã®ä»–é›‘èªŒ
    other_magazine_ids = []
    for publisher_id in ids.get("publisher_ids", [])[:1]:  # æœ€å¤§1å‡ºç‰ˆç¤¾
        exclude_mag = ids.get("magazine_ids", [None])[0] if ids.get("magazine_ids") else None
        publisher_mags = get_publisher_magazines(publisher_id, limit=3, exclude_magazine_id=exclude_mag)
        if publisher_mags:
            extended_info["publisher_magazines"].append(publisher_mags)
            # ä»–é›‘èªŒIDã‚’åé›†
            for node in publisher_mags.get("nodes", []):
                if node.get("type", "").lower() == "magazine":
                    mag_id = node.get("id") or node.get("elementId")
                    if mag_id and mag_id not in other_magazine_ids:
                        other_magazine_ids.append(mag_id)
    
    # 14. ä»–é›‘èªŒã®ä½œå“ã‚°ãƒ©ãƒ•
    if other_magazine_ids:
        other_works = get_magazines_work_graph(other_magazine_ids, work_limit=3, reference_work_id=work_id)
        if other_works:
            extended_info["other_magazines_works"].append(other_works)
    
    return extended_info


def build_graph_context_from_extended(extended_info: Dict[str, Any], query_title: str) -> str:
    """
    15: æ‹¡å¼µã‚°ãƒ©ãƒ•æƒ…å ±ã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—ã‚’æ§‹ç¯‰
    """
    lines: List[str] = []
    
    base_graph = extended_info.get("base_graph", {})
    ids = extended_info.get("extracted_ids", {})
    
    # åŸºæœ¬æƒ…å ±
    work_title = ids.get("work_title") or query_title
    lines.append(f"ã‚¯ã‚¨ãƒª: {work_title}")
    
    # è‘—è€…æƒ…å ±
    base_nodes = base_graph.get("nodes", []) or []
    authors = [n for n in base_nodes if n.get("type", "").lower() == "author"]
    magazines = [n for n in base_nodes if n.get("type", "").lower() == "magazine"]
    publishers = [n for n in base_nodes if n.get("type", "").lower() == "publisher"]
    
    if authors:
        author_name = authors[0].get("properties", {}).get("name") or authors[0].get("name") or authors[0].get("label") or "ä¸æ˜"
        lines.append(f"ã‚¯ã‚¨ãƒªã®ä½œå“ã®ä½œè€…: {author_name}")
    
    if magazines:
        mag_name = magazines[0].get("properties", {}).get("name") or magazines[0].get("name") or magazines[0].get("label") or "ä¸æ˜"
        lines.append(f"ã‚¯ã‚¨ãƒªãŒæ²è¼‰ã•ã‚ŒãŸé›‘èªŒ: {mag_name}")
    
    if publishers:
        pub_name = publishers[0].get("properties", {}).get("name") or publishers[0].get("name") or publishers[0].get("label") or "ä¸æ˜"
        lines.append(f"ã‚¯ã‚¨ãƒªãŒæ²è¼‰ã•ã‚ŒãŸé›‘èªŒã®å‡ºç‰ˆç¤¾: {pub_name}")
    
    # è‘—è€…ã®åˆ¥ä½œå“
    author_works_list = extended_info.get("author_works", [])
    if author_works_list:
        lines.append("")
        author_name = authors[0].get("properties", {}).get("name") or authors[0].get("name") if authors else "ä½œè€…"
        lines.append(f"### {author_name}ã®åˆ¥ä½œå“")
        work_titles_added = set()
        for aw in author_works_list:
            for node in aw.get("nodes", []):
                if node.get("type", "").lower() == "work":
                    title = node.get("properties", {}).get("title") or node.get("title") or node.get("label")
                    if title and title.lower() != work_title.lower() and title not in work_titles_added:
                        lines.append(f"- {title}")
                        work_titles_added.add(title)
        if not work_titles_added:
            lines.append("- ãªã—")
    
    # åŒé›‘èªŒã®åˆ¥ä½œå“
    magazine_works_list = extended_info.get("magazine_works", [])
    if magazine_works_list:
        lines.append("")
        lines.append("### åŒé›‘èªŒã®åˆ¥ä½œå“")
        work_titles_added = set()
        for mw in magazine_works_list:
            nodes_dict = {n.get("id") or n.get("elementId"): n for n in mw.get("nodes", [])}
            for node in mw.get("nodes", []):
                if node.get("type", "").lower() == "work":
                    title = node.get("properties", {}).get("title") or node.get("title") or node.get("label")
                    if title and title.lower() != work_title.lower() and title not in work_titles_added:
                        # ä½œè€…ã‚’æ¢ã™
                        work_author = "ä¸æ˜"
                        for edge in mw.get("edges", []) or mw.get("relationships", []) or []:
                            if edge.get("type") == "created" and edge.get("target") == (node.get("id") or node.get("elementId")):
                                author_node = nodes_dict.get(edge.get("source"))
                                if author_node:
                                    work_author = author_node.get("properties", {}).get("name") or author_node.get("name") or "ä¸æ˜"
                        mag_name = magazines[0].get("properties", {}).get("name") if magazines else "ä¸æ˜"
                        pub_name = publishers[0].get("properties", {}).get("name") if publishers else "ä¸æ˜"
                        lines.append(f"- {title}ï¼ˆä½œè€…: {work_author}ã€é›‘èªŒ: {mag_name}ã€å‡ºç‰ˆç¤¾: {pub_name}ï¼‰")
                        work_titles_added.add(title)
        if not work_titles_added:
            lines.append("- ãªã—")
    
    # åŒå‡ºç‰ˆç¤¾ã®ä»–èªŒã®ä½œå“
    other_mag_works = extended_info.get("other_magazines_works", [])
    if other_mag_works:
        lines.append("")
        lines.append("### åŒå‡ºç‰ˆç¤¾ã®ä»–èªŒã«æ²è¼‰ã•ã‚ŒãŸä½œå“")
        work_titles_added = set()
        for omw in other_mag_works:
            nodes_dict = {n.get("id") or n.get("elementId"): n for n in omw.get("nodes", [])}
            for node in omw.get("nodes", []):
                if node.get("type", "").lower() == "work":
                    title = node.get("properties", {}).get("title") or node.get("title") or node.get("label")
                    if title and title.lower() != work_title.lower() and title not in work_titles_added:
                        # ä½œè€…ã¨é›‘èªŒã‚’æ¢ã™
                        work_author = "ä¸æ˜"
                        work_mag = "ä¸æ˜"
                        for edge in omw.get("edges", []) or omw.get("relationships", []) or []:
                            node_id = node.get("id") or node.get("elementId")
                            if edge.get("type") == "created" and edge.get("target") == node_id:
                                author_node = nodes_dict.get(edge.get("source"))
                                if author_node:
                                    work_author = author_node.get("properties", {}).get("name") or author_node.get("name") or "ä¸æ˜"
                            if edge.get("type") == "published" and edge.get("target") == node_id:
                                mag_node = nodes_dict.get(edge.get("source"))
                                if mag_node:
                                    work_mag = mag_node.get("properties", {}).get("name") or mag_node.get("name") or "ä¸æ˜"
                        pub_name = publishers[0].get("properties", {}).get("name") if publishers else "ä¸æ˜"
                        lines.append(f"- {title}ï¼ˆä½œè€…: {work_author}ã€é›‘èªŒ: {work_mag}ã€å‡ºç‰ˆç¤¾: {pub_name}ï¼‰")
                        work_titles_added.add(title)
                        if len(work_titles_added) >= 5:
                            break
            if len(work_titles_added) >= 5:
                break
        if not work_titles_added:
            lines.append("- ãªã—")
    
    return "\n".join(lines)


def generate_graphrag_recommendation(
    user_input: str,
    context: str,
    token_callback: Optional[Callable[[str], None]] = None,
) -> str:
    """GraphRAGãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‰æ–‡ã‚’ç”Ÿæˆ"""
    rec_prompt = GraphRAGPrompts.get_recommendation_prompt()
    prompt_text = rec_prompt.format(user_query=user_input, context=context)
    
    body = {
        "text": prompt_text,
        "model": "gpt-4.1-nano",
        "temperature": 0.7,
        "max_tokens": 1000,
        "streaming": True,
    }
    
    full_text = ""
    try:
        r = request_with_retry(
            "POST",
            TEXT_GEN_ENDPOINT,
            json=body,
            headers=_auth_headers({"Content-Type": "application/json"}),
            timeout=180,
            stream=True,
        )
        with r:
            r.raise_for_status()
            buffer = ""
            for chunk in r.iter_content(chunk_size=None, decode_unicode=True):
                buffer += chunk
                while "\n\n" in buffer:
                    message, buffer = buffer.split("\n\n", 1)
                    if message.startswith("data: "):
                        line = message[6:].strip()
                        if not line:
                            continue
                        appended = ""
                        try:
                            if line.startswith("{") and line.endswith("}"):
                                data = json.loads(line)
                                if isinstance(data, dict) and "text" in data:
                                    appended = str(data["text"])
                            else:
                                appended = line
                        except Exception:
                            appended = line
                        if appended:
                            full_text += appended
                            if token_callback:
                                token_callback(appended)
    except Exception as e:
        logger.error("GraphRAG generation failed: %s", e)
        return full_text + f"\n[GraphRAGç”Ÿæˆã‚¨ãƒ©ãƒ¼] {e}"
    
    return full_text or "(ç”Ÿæˆçµæœãªã—)"


def run_graphrag_pipeline_new(
    user_input: str,
    token_callback: Optional[Callable[[str], None]] = None,
    selected_title: str | None = None,
) -> Dict[str, Any]:
    """
    æ–°ã—ã„GraphRAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    1-6: ã‚°ãƒ©ãƒ•æ¤œç´¢
    7-10: é¡ä¼¼æ¤œç´¢ï¼ˆå¿…è¦ãªå ´åˆï¼‰
    11-14: æ‹¡å¼µã‚°ãƒ©ãƒ•æƒ…å ±å–å¾—
    15-16: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã¨ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‰ç”Ÿæˆ
    """
    query = selected_title or user_input
    
    # 1-6: ã‚°ãƒ©ãƒ•æ¤œç´¢
    base_graph, search_mode = perform_graph_search(query)
    
    fuzzy_used = False
    similarity_candidates = []
    
    # ã‚°ãƒ©ãƒ•æ¤œç´¢ã§è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆã¯é¡ä¼¼æ¤œç´¢
    if not base_graph.get("nodes"):
        fuzzy_used = True
        # 7-8: ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼æ¤œç´¢
        similarity_candidates = perform_vector_similarity_search(query)
        
        # å€™è£œãŒã‚ã‚‹å ´åˆã€æœ€ä¸Šä½ã®å€™è£œã§å†æ¤œç´¢
        if similarity_candidates:
            best_candidate = similarity_candidates[0]["title"]
            base_graph, search_mode = perform_graph_search(best_candidate)
            query = best_candidate
    
    # 11-14: æ‹¡å¼µã‚°ãƒ©ãƒ•æƒ…å ±å–å¾—
    extended_info = fetch_extended_graph_info(base_graph)
    
    # 15: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
    context = build_graph_context_from_extended(extended_info, query)
    
    # 16: ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‰ç”Ÿæˆ
    recommendation = generate_graphrag_recommendation(user_input, context, token_callback)
    
    return {
        "extracted_title": query,
        "fuzzy_used": fuzzy_used,
        "fuzzy_best_title": similarity_candidates[0]["title"] if similarity_candidates else None,
        "user_selected_candidate": selected_title is not None,
        "search_mode": search_mode,
        "graph_summary": context,
        "graph_debug": json.dumps(extended_info, ensure_ascii=False, indent=2)[:2000],
        "recommendation": recommendation,
        "raw_graph": base_graph,
        "similarity_candidates": similarity_candidates,
    }


def get_similarity_candidates_for_selection(query: str) -> List[Dict[str, Any]]:
    """UIç”¨: é¡ä¼¼æ¤œç´¢ã®å€™è£œã‚’å–å¾—"""
    # ã¾ãšã‚°ãƒ©ãƒ•æ¤œç´¢ã‚’è©¦ã™
    base_graph, _ = perform_graph_search(query)
    if base_graph.get("nodes"):
        return []  # ã‚°ãƒ©ãƒ•ã§è¦‹ã¤ã‹ã£ãŸå ´åˆã¯å€™è£œé¸æŠä¸è¦
    
    # é¡ä¼¼æ¤œç´¢
    candidates = perform_vector_similarity_search(query)
    processed = []
    for c in candidates[:10]:
        score_percent = c.get("score", 0) * 100
        processed.append({
            "title": c["title"],
            "score": c.get("score", 0),
            "display": f"{c['title']} (é¡ä¼¼åº¦: {score_percent:.1f}%)",
        })
    return processed


def stream_generate(text, container, title):
    """APIã‹ã‚‰ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—ã—ã¦è¡¨ç¤º"""
    try:
        api_base = os.getenv("API_BASE", "http://localhost:8000")
        url = f"{api_base}/text-generation/generate"
        headers = _auth_headers({"Content-Type": "application/json"})
        data = {"text": text, "streaming": "true"}

        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å‡¦ç†
        # 502ç­‰ãŒå‡ºã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚ã€æ¥ç¶šç¢ºç«‹ã¾ã§ãƒªãƒˆãƒ©ã‚¤
        # on_retryã§UIã«èµ·å‹•å¾…ã¡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
        def on_retry(ctx: dict):
            wait = ctx.get("wait")
            status = ctx.get("status")
            if status in (502, 503, 504) or status is None:
                container.info(
                    f"ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰èµ·å‹•å¾…ã¡ä¸­... ãƒªãƒˆãƒ©ã‚¤{ctx.get('attempt')}å›ç›®ã€‚"
                    + (f" æ¬¡ã®è©¦è¡Œã¾ã§ç´„{wait:.1f}ç§’" if wait else "")
                )

        response = request_with_retry(
            "POST",
            url,
            json=data,
            headers=headers,
            stream=True,
            timeout=180,
            on_retry=on_retry,
        )
        response.raise_for_status()  # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯

        full_text = ""
        buffer = ""
        with container.container():
            st.subheader(title)
            text_placeholder = st.empty()

            for chunk in response.iter_content(chunk_size=None, decode_unicode=True):
                buffer += chunk
                while "\n\n" in buffer:
                    message, buffer = buffer.split("\n\n", 1)
                    if message.startswith("data: "):
                        line = message[len("data: ") :].strip()
                        if not line:
                            continue

                        # lineãŒJSONå½¢å¼ï¼ˆ"{...}"ï¼‰ã§ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                        if line.startswith("{") and line.endswith("}"):
                            try:
                                json_data = json.loads(line)
                                if isinstance(json_data, dict):
                                    if "text" in json_data:
                                        full_text += str(json_data["text"])
                                    elif "content" in json_data:
                                        full_text += str(json_data["content"])
                                    else:
                                        # ä»–ã®ã‚­ãƒ¼ã‚‚è€ƒæ…®
                                        full_text += " ".join(
                                            [str(v) for v in json_data.values() if isinstance(v, (str, int, float))]
                                        )
                                else:
                                    full_text += str(json_data)
                            except json.JSONDecodeError:
                                # JSONãƒ‡ã‚³ãƒ¼ãƒ‰ã«å¤±æ•—ã—ãŸå ´åˆã¯ã€æ–‡å­—åˆ—ã¨ã—ã¦ãã®ã¾ã¾è¿½åŠ 
                                full_text += line
                        else:
                            # JSONå½¢å¼ã§ãªã„å ´åˆã¯ã€ãã®ã¾ã¾ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦è¿½åŠ 
                            full_text += line

                        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§è¡¨ç¤ºã‚’æ›´æ–°
                        text_placeholder.markdown(full_text)
                        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿æŒã—ã¦å†æç”»æ™‚ã‚‚è¡¨ç¤ºã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
                        st.session_state["raw_llm_output"] = full_text
                        time.sleep(0.01)  # å°‘ã—é…å»¶ã‚’å…¥ã‚Œã¦è¡¨ç¤ºã‚’è¦‹ã‚„ã™ãã™ã‚‹
        # å®Œäº†ãƒ•ãƒ©ã‚°
        st.session_state["raw_llm_done"] = True
    except requests.exceptions.HTTPError as e:
        with container.container():
            st.subheader(title)
            st.error(f"APIå‘¼ã³å‡ºã—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {e.response.status_code}")
            st.text(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {e.response.text}")
        st.session_state["raw_llm_output"] = f"APIã‚¨ãƒ©ãƒ¼: {e.response.status_code}\n{e.response.text}"
        st.session_state["raw_llm_done"] = True

    except requests.exceptions.ConnectionError:
        with container.container():
            st.subheader(title)
            st.error("APIã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“ã€‚API_ServerãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.session_state["raw_llm_output"] = "APIã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
        st.session_state["raw_llm_done"] = True
    except Exception as e:
        with container.container():
            st.subheader(title)
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        st.session_state["raw_llm_output"] = f"ã‚¨ãƒ©ãƒ¼: {str(e)}"
        st.session_state["raw_llm_done"] = True


def main():
    st.title("ğŸ“š GraphRAGã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆãƒ‡ãƒ¢")
    st.markdown("åŒã˜ãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã—ã¦ç´ ã®LLMï¼ˆGraphRAGãªã—ï¼‰ã¨GraphRAGã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆã®çµæœã‚’æ¯”è¼ƒè¡¨ç¤ºã—ã¾ã™ã€‚")
    # å³ä¸‹ã«å°ã•ãªã€Œå‡ºå…¸ã€ãƒªãƒ³ã‚¯ï¼ˆãƒ•ãƒ­ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ï¼‰
    st.markdown(
        """
        <style>
        .floating-citation-link {
            position: fixed;
            right: 16px;
            bottom: 12px;
            background: rgba(255,255,255,0.85);
            backdrop-filter: blur(6px);
            border: 1px solid #e6e6e6;
            border-radius: 8px;
            padding: 4px 8px;
            font-size: 12px;
            z-index: 9999;
            box-shadow: 0 2px 6px rgba(0,0,0,0.08);
        }
        .floating-citation-link a {
            color: #4f46e5;
            text-decoration: none;
        }
        .floating-citation-link a:hover {
            text-decoration: underline;
        }
        </style>
        <div class="floating-citation-link">
            ğŸ”— <a href="/source_link" target="_self">å‡ºå…¸</a>
        </div>
        """,
        unsafe_allow_html=True,
    )

    # å…¥åŠ›æ¬„ + å·»æ•°ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆPCã§ã¯æ¨ªä¸¦ã³ 4:1 / ãƒ¢ãƒã‚¤ãƒ«ã§ã¯è‡ªå‹•ç¸¦ç©ã¿ï¼‰
    st.subheader("ğŸ”¤ æ¼«ç”»å…¥åŠ›ã¨ãƒ•ã‚£ãƒ«ã‚¿")
    col_title, col_vol = st.columns([4, 1], gap="small")
    with col_title:
        input_text = st.text_area(
            "ãŠã™ã™ã‚æ–‡ã‚’ç”Ÿæˆã—ãŸã„æ¼«ç”»åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚:",
            height=100,
            placeholder="ä¾‹: NARUTO",
        )
    with col_vol:
        min_vol = st.number_input(
            "nå·»ä»¥ä¸Šç™ºè¡Œ (â‰¤10)",
            min_value=1,
            max_value=10,
            value=5,
            step=1,
            help="æŒ‡å®šã—ãŸå·»æ•°ä»¥ä¸Šã®å˜è¡Œæœ¬ãŒç™ºè¡Œã•ã‚Œã¦ã„ã‚‹ä½œå“ã«é™å®šã—ã¾ã™",
        )

    # æ¯”è¼ƒç”¨ã«ç´ ã®LLMã‚’å®Ÿè¡Œã™ã‚‹ã‹ã®åˆ‡ã‚Šæ›¿ãˆ
    show_raw_llm = st.checkbox(
        "ç´ ã®LLMï¼ˆGraphRAGãªã—ï¼‰ã‚‚å®Ÿè¡Œã—ã¦æ¯”è¼ƒã™ã‚‹",
        value=True,
        help="ã‚ªãƒ•ã«ã™ã‚‹ã¨ç´ ã®LLMã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦GraphRAGã®ã¿å®Ÿè¡Œã—ã¾ã™",
    )

    # å³ã‚«ãƒ©ãƒ ã«GraphRAGã®çµæœã‚’æ›¸ãè¾¼ã‚€ãƒ˜ãƒ«ãƒ‘ãƒ¼
    def run_graphrag_into(
        right_container,
        status_text,
        progress_bar,
        user_text: str,
        min_volumes: int,
        selected_title: str | None = None,
    ):
        status_text.text("ğŸ”„ GraphRAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œä¸­...")
        progress_bar.progress(60)
        with right_container:
            st.subheader("ğŸ•¸ï¸ GraphRAGã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆ")
            with st.spinner("Graph / æ¨è–¦ç”Ÿæˆä¸­..."):
                try:
                    reco_placeholder = st.empty()
                    buffer = []

                    def on_token(t: str):
                        buffer.append(t)
                        if "\n" in t or len(buffer) % 5 == 0 or t.endswith(("ã€‚", "!", "?")):
                            reco_placeholder.markdown("".join(buffer))

                    result = run_graphrag_pipeline_new(
                        user_text,
                        token_callback=on_token,
                        selected_title=selected_title,
                    )
                    reco_placeholder.markdown(result["recommendation"])
                    with st.expander("æŠ½å‡ºãƒ»æ¤œç´¢ãƒ¡ã‚¿æƒ…å ±"):
                        st.write(
                            {
                                "extracted_title": result.get("extracted_title"),
                                "search_mode": result.get("search_mode"),
                                "fuzzy_used": result.get("fuzzy_used"),
                                "fuzzy_best_title": result.get("fuzzy_best_title"),
                                "user_selected_candidate": result.get("user_selected_candidate"),
                                "node_count": len(result.get("raw_graph", {}).get("nodes", []) or []),
                                "relationship_count": len(result.get("raw_graph", {}).get("edges", []) or []),
                            }
                        )
                        st.caption("ã‚°ãƒ©ãƒ•ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ")
                        st.text(result.get("graph_summary"))
                except ValueError as e:
                    st.error(str(e))
                except Exception as e:  # noqa: BLE001
                    st.error(f"GraphRAGå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        progress_bar.progress(90)
        progress_bar.progress(100)
        status_text.text("âœ… ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        time.sleep(1)
        progress_bar.empty()
        status_text.empty()
        st.success("âœ… ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")

    # ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ï¼ˆãƒšãƒ¼ã‚¸å†…ï¼‰å€™è£œé¸æŠãƒ‘ãƒãƒ«
    def render_candidate_selector_panel(right_container):  # uses session_state
        cands = st.session_state.get("fuzzy_candidates", [])
        base_query = st.session_state.get("dialog_extracted_title") or st.session_state.get("pending_user_input")
        with right_container:
            st.subheader("ğŸ” å€™è£œãŒè¤‡æ•°è¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
            st.write("æ­£ã—ã„ä½œå“ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚é¸æŠå¾Œã«ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™ã€‚")
            st.caption(f"æ¤œç´¢èª: {base_query}")
            st.caption(f"å€™è£œä»¶æ•°: {len(cands)} ä»¶")

            if not cands:
                st.info("å€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ¤œç´¢æ¡ä»¶ã‚’å¤‰ãˆã¦ãŠè©¦ã—ãã ã•ã„ã€‚")
                return

            options = [c["display"] for c in cands]
            idx = st.radio(
                "å€™è£œ",
                options=range(len(options)),
                format_func=lambda i: options[i],
                index=0,
                key="cand_idx",
            )
            cols = st.columns([1, 1])
            with cols[0]:
                if st.button("ã“ã®ä½œå“ã§ç”Ÿæˆã™ã‚‹", type="primary"):
                    chosen = cands[idx]
                    st.session_state["chosen_title"] = chosen["title"]
                    st.session_state["awaiting_candidate_selection"] = False
                    st.session_state["start_generation"] = True
                    st.rerun()
            with cols[1]:
                if st.button("ä¸Šä½å€™è£œã§ç”Ÿæˆ"):
                    # ä¸Šä½å€™è£œã¾ãŸã¯æŠ½å‡ºã‚¿ã‚¤ãƒˆãƒ«ã§ç¶šè¡Œ
                    fallback = cands[0]["title"] if cands else (st.session_state.get("dialog_extracted_title") or "")
                    st.session_state["chosen_title"] = fallback
                    st.session_state["awaiting_candidate_selection"] = False
                    st.session_state["start_generation"] = True
                    st.rerun()

    # æ—§ãƒ•ãƒ©ã‚°ï¼ˆãƒ¢ãƒ¼ãƒ€ãƒ«ç”¨ï¼‰ãŒæ®‹ã£ã¦ã„ã‚Œã°æ–°ãƒ•ãƒ©ã‚°ã«ç§»è¡Œ
    if st.session_state.get("open_candidate_dialog"):
        st.session_state["awaiting_candidate_selection"] = True
        del st.session_state["open_candidate_dialog"]

    # é¸æŠå¾…ã¡ãªã‚‰ã€ç”ŸLLMçµæœã‚’å·¦ã«ä¿æŒè¡¨ç¤ºã—ã¤ã¤ã€å€™è£œé¸æŠãƒ‘ãƒãƒ«ã‚’å‡ºã™ï¼ˆGraphRAGã¯æœªå®Ÿè¡Œï¼‰
    if st.session_state.get("awaiting_candidate_selection"):
        st.markdown("---")
        st.subheader("ğŸ“Š ç”Ÿæˆçµæœã®æ¯”è¼ƒ")
        col1, col2 = st.columns(2)
        with col1.container():
            st.subheader("ğŸ’¬ ç´ ã®LLMï¼ˆGraphRAGãªã—ï¼‰")
            raw_out = st.session_state.get("raw_llm_output")
            if raw_out:
                st.markdown(raw_out)
            else:
                st.info("ç´ ã®LLMã®çµæœã¯ã“ã“ã«è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
        with col2.container():
            st.subheader("ğŸ•¸ï¸ GraphRAGã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆ")
            st.info("å€™è£œã‚’é¸æŠã™ã‚‹ã¨GraphRAGã®ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™ã€‚")
        st.markdown("---")
        render_candidate_selector_panel(col2.container())
        st.stop()

    # å®Ÿè¡Œãƒœã‚¿ãƒ³æŠ¼ä¸‹æ™‚ã®å‡¦ç†ï¼ˆã¾ãšç´ ã®LLMâ†’ãã®å¾Œã«ã‚°ãƒ©ãƒ•æ¤œç´¢/é¡ä¼¼æ¤œç´¢â†’å¿…è¦ãªã‚‰å€™è£œé¸æŠâ†’GraphRAGï¼‰
    if st.button("ğŸš€ ç”Ÿæˆé–‹å§‹", type="primary", use_container_width=True):
        if not input_text.strip():
            st.warning("âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        else:
            try:
                # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼ˆæ¯”è¼ƒè¡¨ç¤ºï¼‰ã¨ç”Ÿæˆ
                st.markdown("---")
                st.subheader("ğŸ“Š ç”Ÿæˆçµæœã®æ¯”è¼ƒ")
                col1, col2 = st.columns(2)
                progress_bar = st.progress(0)
                status_text = st.empty()

                if show_raw_llm:
                    with col1.container():
                        prompt = get_standard_recommend_prompt(input_text)
                        stream_generate(prompt, col1, "ğŸ’¬ ç´ ã®LLMï¼ˆGraphRAGãªã—ï¼‰")

                # æ›–æ˜§æ€§è§£æ¶ˆï¼ˆå€™è£œé¸æŠï¼‰ã‚’å®Œäº†ã•ã›ã‚‹ã€‚è§£æ±ºå¾Œã«ç”Ÿæˆã‚’é–‹å§‹ã™ã‚‹ã€‚
                # ã‚¹ãƒ”ãƒŠãƒ¼ã¨çµæœUIã¯å³ã‚«ãƒ©ãƒ ã«è¡¨ç¤º
                with col2.container():
                    with st.spinner("ã‚°ãƒ©ãƒ•ã‹ã‚‰æ¼«ç”»åã‚’æ¤œç´¢ä¸­..."):
                        # 1-6) ã‚°ãƒ©ãƒ•æ¤œç´¢ã‚’æ®µéšçš„ã«å®Ÿè¡Œ
                        graph_result, search_mode = perform_graph_search(input_text)

                        selected_title_for_run: str | None = None
                        processed = []
                        
                        if graph_result.get("nodes"):
                            # ã‚°ãƒ©ãƒ•æ¤œç´¢ã§è¦‹ã¤ã‹ã£ãŸ
                            selected_title_for_run = None  # å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã§ãã®ã¾ã¾å®Ÿè¡Œ
                        else:
                            # 7-8) ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼æ¤œç´¢
                            st.markdown(
                                "ğŸ” **:red[ä¸€è‡´ã™ã‚‹æ¼«ç”»ä½œå“ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€è¿‘ãã†ãªæ¼«ç”»ä½œå“åã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¾ã™ã€‚]**"
                            )
                            candidates = perform_vector_similarity_search(input_text)
                            
                            for c in candidates[:10]:
                                score_percent = c.get("score", 0) * 100
                                processed.append({
                                    "title": c["title"],
                                    "score": c.get("score", 0),
                                    "display": f"{c['title']} (é¡ä¼¼åº¦: {score_percent:.1f}%)",
                                })

                    # æ›–æ˜§æ€§ã®çµæœã«å¿œã˜ã¦åˆ†å²
                    if len(processed) > 1:
                        # 9) 2ä»¶ä»¥ä¸Š â†’ ãƒšãƒ¼ã‚¸å†…ãƒ‘ãƒãƒ«ã§é¸æŠã€é¸æŠå¾Œã«ç”Ÿæˆé–‹å§‹
                        st.session_state["fuzzy_candidates"] = processed
                        st.session_state["dialog_extracted_title"] = input_text
                        st.session_state["awaiting_candidate_selection"] = True
                        st.session_state["pending_user_input"] = input_text
                        st.session_state["pending_min_vol"] = int(min_vol)
                        st.session_state["pending_show_raw_llm"] = bool(show_raw_llm)
                        # ç¾åœ¨ã®ãƒ©ãƒ³ã§å³ã‚«ãƒ©ãƒ ã«ãƒ‘ãƒãƒ«è¡¨ç¤ºã¸ç§»è¡Œ
                        st.markdown("---")
                        render_candidate_selector_panel(col2.container())
                        st.stop()
                    else:
                        # å€™è£œ0/1ä»¶ â†’ ãã®ã¾ã¾ç”Ÿæˆé–‹å§‹
                        if processed:
                            selected_title_for_run = processed[0]["title"]
                        # 10) é¸æŠã•ã‚ŒãŸå€™è£œã§ã‚°ãƒ©ãƒ•æ¤œç´¢ã—ã¦ä»¥é™ã®å‡¦ç†ã‚’å®Ÿè¡Œ

                    run_graphrag_into(
                        col2.container(),
                        status_text,
                        progress_bar,
                        input_text,
                        int(min_vol),
                        selected_title=selected_title_for_run,
                    )
            except Exception as e:
                st.error(f"å‰å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    # é¸æŠå¾Œã«è‡ªå‹•å®Ÿè¡Œï¼ˆå·¦ã«ç´ ã®LLMçµæœã‚’å†æ²ï¼‰
    if st.session_state.get("start_generation"):
        # é¸æŠå¾Œã¯æ¯”è¼ƒè¡¨ç¤ºã‚’å†æ§‹ç¯‰ã—ã¦ç”Ÿæˆ
        st.markdown("---")
        st.subheader("ğŸ“Š ç”Ÿæˆçµæœã®æ¯”è¼ƒ")
        col1, col2 = st.columns(2)
        progress_bar = st.progress(0)
        status_text = st.empty()

        # å·¦ã«ä¿å­˜æ¸ˆã¿ã®ç´ ã®LLMçµæœã‚’è¡¨ç¤ºï¼ˆå†ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯ã—ãªã„ï¼‰
        with col1.container():
            st.subheader("ğŸ’¬ ç´ ã®LLMï¼ˆGraphRAGãªã—ï¼‰")
            raw_out = st.session_state.get("raw_llm_output")
            if raw_out:
                st.markdown(raw_out)
            else:
                st.info("ç´ ã®LLMã®çµæœã¯ã“ã“ã«è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")

        run_graphrag_into(
            col2.container(),
            status_text,
            progress_bar,
            st.session_state.get("pending_user_input", input_text),
            st.session_state.get("pending_min_vol", int(min_vol)),
            selected_title=st.session_state.get("chosen_title"),
        )
        # å¾Œç‰‡ä»˜ã‘ï¼ˆãƒ¢ãƒ¼ãƒ€ãƒ«ã‚’é–‰ã˜ãŸã¾ã¾ã«ï¼‰
        for k in [
            "fuzzy_candidates",
            "dialog_extracted_title",
            "awaiting_candidate_selection",
            "pending_user_input",
            "pending_min_vol",
            "pending_show_raw_llm",
            "chosen_title",
            "start_generation",
        ]:
            if k in st.session_state:
                del st.session_state[k]

    # APIã‚µãƒ¼ãƒãƒ¼ã®çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯
    st.markdown("---")
    st.subheader("ğŸ”§ ã‚µãƒ¼ãƒãƒ¼çŠ¶æ…‹")

    if st.button("ã‚µãƒ¼ãƒãƒ¼æ¥ç¶šç¢ºèª"):
        check_server_connection(os.getenv("API_BASE", "http://localhost:8000"))


def check_server_connection(api_base: str):
    try:
        response = request_with_retry("GET", f"{api_base}/health", headers=_auth_headers(), timeout=5)
        if response.status_code == 200:
            st.success("âœ… APIã‚µãƒ¼ãƒãƒ¼ã«æ­£å¸¸ã«æ¥ç¶šã§ãã¾ã™")
        else:
            st.warning(f"âš ï¸ ã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰ã®å¿œç­”ãŒç•°å¸¸ã§ã™ (ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status_code})")
    except requests.exceptions.ConnectionError:
        st.error("âŒ APIã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“ã€‚API_ServerãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    except Exception as e:
        st.error(f"âŒ æ¥ç¶šç¢ºèªä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")


def get_standard_recommend_prompt(user_query: str) -> str:
    prompt_template = StandardMangaPrompts.get_recommendation_prompt()
    return prompt_template.format(user_query=user_query)


if __name__ == "__main__":
    main()
